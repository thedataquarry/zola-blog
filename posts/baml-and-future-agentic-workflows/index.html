<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Why I'm excited about BAML and the future of agentic workflows | The Data Quarry</title><meta content="Why I'm excited about BAML and the future of agentic workflows" property=og:title><meta content="Prashanth Rao" name=author><meta content=en_US property=og:locale><meta content="How BAML's innovations on structured generation with LLMs allows you to build more reliable agents, chatbots with RAG, and more." name=description><meta content="How BAML's innovations on structured generation with LLMs allows you to build more reliable agents, chatbots with RAG, and more." property=og:description><link href=https://thedataquarry.github.io/posts/baml-and-future-agentic-workflows/ rel=canonical><meta content=https://thedataquarry.github.io/posts/baml-and-future-agentic-workflows/ property=og:url><meta content="The Data Quarry" property=og:site_name><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=og:image><meta content=article property=og:type><meta content=2025-01-29T00:00:00+00:00 property=article:published_time><meta " content=summary_large_image name=twitter:card><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=twitter:image><meta content="Why I'm excited about BAML and the future of agentic workflows" property=twitter:title><meta content=@tech_optimist name=twitter:site><meta content="How BAML's innovations on structured generation with LLMs allows you to build more reliable agents, chatbots with RAG, and more." name=description><title>Why I'm excited about BAML and the future of agentic workflows</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#376dd9;--primary-pale-color:#698bcf1c;--inline-code-color:#444;--text-color:#444;--text-pale-color:#545967;--bg-color:#fff;--highlight-mark-color:#5f75b045;--callout-note-color:#3e70d6;--callout-important-color:#7a46cd;--callout-warning-color:#d3822b;--callout-alert-color:#d43f3f;--callout-question-color:#3089b5;--callout-tip-color:#35a06b;--main-font:"IBMPlexSans",ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:"IBMPlexMono",ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:750px;--main-max-width:750px;--avatar-size:175px;--paragraph-font-size:16px;--paragraph-line-height:1.5em;--aside-font-size:16px;--img-border-radius:4px;--inline-code-border-radius:2px}body.dark{--primary-color:#689afd;--primary-pale-color:#93acdd1c;--inline-code-color:#d2d2d2;--text-color:#ddd;--text-pale-color:#a0a0a0;--bg-color:#202124;--highlight-mark-color:#5f75b045;--callout-note-color:#698bcf;--callout-important-color:#9374c5;--callout-warning-color:#c99054;--callout-alert-color:#d35757;--callout-question-color:#5091b2;--callout-tip-color:#3ea06f}</style><link href=/main.css rel=stylesheet><script async data-website-id=01228822-c189-4b8a-93ba-733d045bf346 src=https://analytics.eu.umami.is/script.js></script><body class=post><script>if(localStorage.getItem('theme')=='dark'){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a href=/>The Data Quarry</a><button aria-label="toggle expand" class=separator id=toggler>::</button><span class="wrap left fold">{</span><a href=/posts>blog</a><span class="wrap-separator fold">,</span><a class=fold href=/talks>talks</a><span class="wrap-separator fold">,</span><a class=fold href=/projects>projects</a><span class="wrap right fold">} ;</span></nav><div id=btns><a rel="noreferrer noopener" aria-label=GitHub href=https://github.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=Twitter href=https://twitter.com/tech_optimist target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Twitter</title><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=LinkedIn href=https://www.linkedin.com/in/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill=currentColor></path></svg> </a><a aria-label="Buy me a coffee" rel="noreferrer noopener" href=https://www.buymeacoffee.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Buy Me A Coffee</title><path d="M20.216 6.415l-.132-.666c-.119-.598-.388-1.163-1.001-1.379-.197-.069-.42-.098-.57-.241-.152-.143-.196-.366-.231-.572-.065-.378-.125-.756-.192-1.133-.057-.325-.102-.69-.25-.987-.195-.4-.597-.634-.996-.788a5.723 5.723 0 00-.626-.194c-1-.263-2.05-.36-3.077-.416a25.834 25.834 0 00-3.7.062c-.915.083-1.88.184-2.75.5-.318.116-.646.256-.888.501-.297.302-.393.77-.177 1.146.154.267.415.456.692.58.36.162.737.284 1.123.366 1.075.238 2.189.331 3.287.37 1.218.05 2.437.01 3.65-.118.299-.033.598-.073.896-.119.352-.054.578-.513.474-.834-.124-.383-.457-.531-.834-.473-.466.074-.96.108-1.382.146-1.177.08-2.358.082-3.536.006a22.228 22.228 0 01-1.157-.107c-.086-.01-.18-.025-.258-.036-.243-.036-.484-.08-.724-.13-.111-.027-.111-.185 0-.212h.005c.277-.06.557-.108.838-.147h.002c.131-.009.263-.032.394-.048a25.076 25.076 0 013.426-.12c.674.019 1.347.067 2.017.144l.228.031c.267.04.533.088.798.145.392.085.895.113 1.07.542.055.137.08.288.111.431l.319 1.484a.237.237 0 01-.199.284h-.003c-.037.006-.075.01-.112.015a36.704 36.704 0 01-4.743.295 37.059 37.059 0 01-4.699-.304c-.14-.017-.293-.042-.417-.06-.326-.048-.649-.108-.973-.161-.393-.065-.768-.032-1.123.161-.29.16-.527.404-.675.701-.154.316-.199.66-.267 1-.069.34-.176.707-.135 1.056.087.753.613 1.365 1.37 1.502a39.69 39.69 0 0011.343.376.483.483 0 01.535.53l-.071.697-1.018 9.907c-.041.41-.047.832-.125 1.237-.122.637-.553 1.028-1.182 1.171-.577.131-1.165.2-1.756.205-.656.004-1.31-.025-1.966-.022-.699.004-1.556-.06-2.095-.58-.475-.458-.54-1.174-.605-1.793l-.731-7.013-.322-3.094c-.037-.351-.286-.695-.678-.678-.336.015-.718.3-.678.679l.228 2.185.949 9.112c.147 1.344 1.174 2.068 2.446 2.272.742.12 1.503.144 2.257.156.966.016 1.942.053 2.892-.122 1.408-.258 2.465-1.198 2.616-2.657.34-3.332.683-6.663 1.024-9.995l.215-2.087a.484.484 0 01.39-.426c.402-.078.787-.212 1.074-.518.455-.488.546-1.124.385-1.766zm-1.478.772c-.145.137-.363.201-.578.233-2.416.359-4.866.54-7.308.46-1.748-.06-3.477-.254-5.207-.498-.17-.024-.353-.055-.47-.18-.22-.236-.111-.71-.054-.995.052-.26.152-.609.463-.646.484-.057 1.046.148 1.526.22.577.088 1.156.159 1.737.212 2.48.226 5.002.19 7.472-.14.45-.06.899-.13 1.345-.21.399-.072.84-.206 1.08.206.166.281.188.657.162.974a.544.544 0 01-.169.364zm-6.159 3.9c-.862.37-1.84.788-3.109.788a5.884 5.884 0 01-1.569-.217l.877 9.004c.065.78.717 1.38 1.5 1.38 0 0 1.243.065 1.658.065.447 0 1.786-.065 1.786-.065.783 0 1.434-.6 1.499-1.38l.94-9.95a3.996 3.996 0 00-1.322-.238c-.826 0-1.491.284-2.26.613z" fill=currentColor></path></svg> </a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside class=blur><nav><ul><li><a class=h2 href=#what-is-baml-and-why-should-we-care>What is BAML and why should we care?</a><li><a class=h2 href=#what-do-we-mean-by-agents>What do we mean by “agents”?</a><li><a class=h2 href=#limitations-of-structured-generation-with-llms>Limitations of structured generation with LLMs</a><li><a class=h2 href=#how-baml-works-under-the-hood>How BAML works under the hood</a><li><a class=h2 href=#strengths-of-baml>Strengths of BAML</a> <ul><li><a class=h3 href=#right-level-of-abstraction>Right level of abstraction</a><li><a class=h3 href=#tight-iterative-feedback-loop>Tight iterative feedback loop</a><li><a class=h3 href=#language-agnosticity>Language agnosticity</a><li><a class=h3 href=#lossless-compression-in-token-space>Lossless compression in token space</a></ul><li><a class=h2 href=#baml-s-philosophy>BAML’s philosophy</a><li><a class=h2 href=#but-wait-didn-t-we-say-agents>But wait, didn’t we say agents?</a><li><a class=h2 href=#conclusions>Conclusions</a><li><a class=h2 href=#learning-resources>Learning resources</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Why I'm excited about BAML and the future of agentic workflows</h1><div id=post-info><div id=date><span id=publish>2025-01-29</span></div><div id=tags><a href=https://thedataquarry.github.io/tags/baml><span>#</span>baml</a><a href=https://thedataquarry.github.io/tags/agentic><span>#</span>agentic</a><a href=https://thedataquarry.github.io/tags/workflow><span>#</span>workflow</a><a href=https://thedataquarry.github.io/tags/orchestration><span>#</span>orchestration</a><a href=https://thedataquarry.github.io/tags/rust><span>#</span>rust</a></div></div><h2 id=what-is-baml-and-why-should-we-care>What is BAML and why should we care?<a aria-label="Anchor link for: what-is-baml-and-why-should-we-care" class=zola-anchor href=#what-is-baml-and-why-should-we-care>#</a></h2><p>It’s a new year, and judging by the frenzy of new AI frameworks over the last several months, I think it’s fair to state that 2025 is going to largely be about agents and AI workflow orchestration. In 2024, some well-known companies pivoted<sup class=footnote-reference><a href=#1>1</a></sup>,<sup class=footnote-reference><a href=#2>2</a></sup> toward enabling better agentic workflows and observability and monitoring. Having dabbled in a variety of existing frameworks over the last several months, I always felt something was lacking. Either some functionality was missing, code felt too verbose or was too highly abstracted, or the API was too rigid. It always felt like wearing a straitjacket when trying to build even the most simple orchestrations and workflows.<p>For me, all this changed when I recently discovered <a rel="nofollow noreferrer" href=https://docs.boundaryml.com/home>BAML</a>:<blockquote><p>BAML is a domain-specific language (DSL) to generate structured outputs from LLMs — with the best developer experience. With BAML you can build reliable Agents, Chatbots with RAG, extract data from PDFs, and more.</blockquote><p>The part that says “best developer experience” is particularly important from my perspective, and is why I believe BAML is going to rapidly gain in popularity in 2025. Although there have been new agentic and AI workflow orchestration frameworks coming out seemingly every month lately – I think that BAML stands out from the rest of the pack. While these may seem like tall and potentially premature claims, I hope that this post (at the very least) inspires some curiosity in you to begin building with it.<h2 id=what-do-we-mean-by-agents>What do we mean by “agents”?<a aria-label="Anchor link for: what-do-we-mean-by-agents" class=zola-anchor href=#what-do-we-mean-by-agents>#</a></h2><p>Before getting into BAML and its role in the agentic and AI workflow orchestration ecosystem, it’s worth describing what we mean when we use the terms “agent” or “agentic workflows”. I like the definition given by Hugging Face <a rel="nofollow noreferrer" href=https://huggingface.co/blog/smolagents#%F0%9F%A4%94-what-are-agents>in their blog</a>:<blockquote><p>AI Agents are programs where LLM outputs control the workflow.</blockquote><p>Whether or not you agree with the definition above, there’s no doubt that the term “agent” is commonly used synonymously with parts of the system that use LLMs to dictate the next action. The level of autonomy and the complexity of steps may vary in your case, but the core idea is that agentic workflows of today use LLMs at various stages for decision making and control flow.<p>The biggest problem that developers face when building LLM-based or agentic workflows is a lack of determinism in the outcome. Things can fail quite spectacularly in production, in large part because these workflows can break in many ways, and a lot of the time, issues tend to surface only once the system is already in production. BAML aims to address this by using a powerful, concise type system. More on this below.<h2 id=limitations-of-structured-generation-with-llms>Limitations of structured generation with LLMs<a aria-label="Anchor link for: limitations-of-structured-generation-with-llms" class=zola-anchor href=#limitations-of-structured-generation-with-llms>#</a></h2><p>JSON is an amazing format for passing data around via REST APIs. LLMs that are required to produce JSON need special treatment<sup class=footnote-reference><a href=#3>3</a></sup> to control their token generation in a way that ensures the JSON is valid. Also, not all LLMs support structured output generation. Because LLMs output text a token at a time and most of them do not deterministically output valid JSON or other stuctured formats, most frameworks approach this by explicitly reprompting the LLM to redo the task if it fails. Every now and then, the LLM might miss double quotes in the key/value of the JSON entry, resulting in the entire JSON failing to parse.<figure><img alt="LLMs are great at outputing strings, not JSON (unless you control generation)" loading=lazy src=baml-1.png><figcaption>LLMs are great at outputing strings, not JSON (unless you control generation)</figcaption></figure><h2 id=how-baml-works-under-the-hood>How BAML works under the hood<a aria-label="Anchor link for: how-baml-works-under-the-hood" class=zola-anchor href=#how-baml-works-under-the-hood>#</a></h2><p>The BAML toolchain is designed to address the problem of consistent and reliable structured generation with LLMs. It improves reliability in your AI workflows, while also being faster <em>and</em> cheaper.<p>Upstream, BAML massively improves the prompt engineering experience by providing a type-safe domain-specific language (DSL), and generates compact prompts that are easy to write, read and test.<p>Downstream, the BAML parser obtains the LLM’s output and applies <em>post facto</em> fixes and validation to the output. Instead of relying on costly methods like re-prompting the LLM to fix minor issues in the outputs (which takes seconds), the BAML engine corrects the output in milliseconds, thus saving money on API calls while also allowing you to use smaller, cheaper models that can achieve largely the same outcome as bigger, more expensive models<sup class=footnote-reference><a href=#4>4</a></sup>.<figure><img alt="BAML's upstream and downstream prompting and parsing workflow" loading=lazy src=baml-5.png><figcaption>BAML's upstream and downstream prompting and parsing workflow</figcaption></figure><p>The BAML parser uses a technique called <a rel="nofollow noreferrer" href=https://www.boundaryml.com/blog/schema-aligned-parsing#sap>Schema-Aligned Parsing</a> (SAP) to fix the LLM’s output via a rule-based engine, applying error correction techniques to get the output to conform to the known schema. Because the parser is written in Rust 🦀, it’s able to apply the fixes in &LT10 ms, which is orders of magnitude faster (and cheaper) than re-prompting the LLM to fix its mistakes. It’s an <em>engineering solution</em> to a problem that shouldn’t require the sophistication of a full-fledged LLM. Even if LLMs improve remarkably over the coming years, which they likely will, token generation per a strict format would still be more expensive than a rule-based parser, which is <strong>free</strong>.<h2 id=strengths-of-baml>Strengths of BAML<a aria-label="Anchor link for: strengths-of-baml" class=zola-anchor href=#strengths-of-baml>#</a></h2><p>From my perspective as someone who cares deeply about developer experience, BAML achieves a stellar design. This is due to a combination of the features described below.<h3 id=right-level-of-abstraction>Right level of abstraction<a aria-label="Anchor link for: right-level-of-abstraction" class=zola-anchor href=#right-level-of-abstraction>#</a></h3><p>All logic and prompts are transparent and customizable at <strong>just one directory deep</strong> and not hidden away from the developer in obscure files somewhere inside the code base. Prompts that are modified by an internal function but are not transparent to the developer are just bugs waiting to happen!<p>Here’s how it would look once you initialize a new BAML project:<pre style=background:#2e3440;color:#d8dee9><code><span>./my_project/
</span><span>├── baml_src/
</span><span>    ├── clients.baml
</span><span>    ├── generators.baml
</span><span>    └── models.baml
</span><span>├── baml_client/
</span><span>    ├── ...
</span><span>    ├── ...
</span><span>    └── ...
</span></code></pre><p>The file <code>clients.baml</code> contains the settings and the fallback logic for the LLMs used in the project. <code>generators.baml</code> contains the information on the codegen component in your client language of choice (e.g. Python, TypeScript, etc.). And finally, <code>models.baml</code> and any other files like it contain the data models, tools, prompts and tests used in the project. A typical project might have many components that power different parts of the workflow, so any number of additional <code>.baml</code> files can be added to the <code>baml_src/</code> directory to separate things out cleanly.<figure><img alt="The structure of a BAML project" loading=lazy src=baml-2.png><figcaption>The structure of a BAML project</figcaption></figure><p>The <code>baml_client/</code> directory contains the generated code in the application language of choice for the project. This is the library code that you can use in your application (that leverages BAML under the hood to power the workflow). Keeping the relevant files cleanly separated with full transparency and a high degree of customizability is why I think BAML hits upon the perfect level of abstraction.<h3 id=tight-iterative-feedback-loop>Tight iterative feedback loop<a aria-label="Anchor link for: tight-iterative-feedback-loop" class=zola-anchor href=#tight-iterative-feedback-loop>#</a></h3><p>Prompts in BAML are immediately testable, as is common in software engineering workflows, with the LLM’s inputs and outputs being very easy to review by a human over a range of test cases. This makes prompt engineering a breeze!<figure><img alt="The model and functions in BAML (left) and the actual prompt sent to the LLM (right)" loading=lazy src=baml-3.png><figcaption>The model and functions in BAML (left) and the actual prompt sent to the LLM (right)</figcaption></figure><p>The example above shows how to use BAML’s expressive type system to extract structured data from a resume. Models are defined using the <code>class</code> keyword. The <code>Experience</code> class contains the person’s role, company, start dates and end dates of employment. <code>Resume</code> consists of a person’s name, a list of their experiences (which are each instances of the <code>Experience</code> class) and a list of skills. Note how the schema in the prompt that BAML sends to the LLM isn’t stringified JSON, but rather a more concise version without double quotes. The comment lines beginning with <code>//</code> were provided by the <code>description</code> annotation when specifying the model in BAML. This helps the LLM better understand the task. Template strings that use the Jinja templating language provide a way to insert variables and expressions into the prompt.<p>The moment the developer writes a BAML function and defines the prompt, the resulting prompt generated by BAML <em>is immediately visible and testable in the editor</em>.<p>Here’s an example test function in BAML for my resume – it’s super easy to read, write and run the test.<pre class=language-rs data-lang=rs style=background:#2e3440;color:#d8dee9><code class=language-rs data-lang=rs><span>test my_resume {
</span><span>  functions [ExtractResume]
</span><span>  args {
</span><span>    resume </span><span style=color:#81a1c1>#</span><span style=color:#a3be8c>"
</span><span style=color:#a3be8c>      Prashanth Rao
</span><span style=color:#a3be8c>
</span><span style=color:#a3be8c>      Experience:
</span><span style=color:#a3be8c>      - AI Engineer at Kùzu Inc. (2024 Jan - Present)
</span><span style=color:#a3be8c>      - AI and Data Engineer at RBC (2022 May - 2023 Dec)
</span><span style=color:#a3be8c>
</span><span style=color:#a3be8c>      Skills:
</span><span style=color:#a3be8c>      - Python
</span><span style=color:#a3be8c>      - Rust
</span><span style=color:#a3be8c>    "</span><span style=color:#81a1c1>#
</span><span>  }
</span><span>}
</span></code></pre><p>And here is the result obtained via a call to <code>gpt-4o-mini</code>, which is perfectly valid JSON, with dates in the desired format as requested in the prompt. All of this was done before a line of application code was written (right in the editor)!<pre class=language-json data-lang=json style=background:#2e3440;color:#d8dee9><code class=language-json data-lang=json><span>{
</span><span>  </span><span style=color:#a3be8c>"name"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"Prashanth Rao"</span><span style=color:#eceff4>,
</span><span>  </span><span style=color:#a3be8c>"experience"</span><span style=color:#eceff4>: </span><span>[
</span><span>    {
</span><span>      </span><span style=color:#a3be8c>"role"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"AI Engineer"</span><span style=color:#eceff4>,
</span><span>      </span><span style=color:#a3be8c>"company"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"Kùzu Inc."</span><span style=color:#eceff4>,
</span><span>      </span><span style=color:#a3be8c>"start_date"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"2024-01"</span><span style=color:#eceff4>,
</span><span>      </span><span style=color:#a3be8c>"end_date"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"Unknown"
</span><span>    }</span><span style=color:#eceff4>,
</span><span>    {
</span><span>      </span><span style=color:#a3be8c>"role"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"AI and Data Engineer"</span><span style=color:#eceff4>,
</span><span>      </span><span style=color:#a3be8c>"company"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"RBC"</span><span style=color:#eceff4>,
</span><span>      </span><span style=color:#a3be8c>"start_date"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"2022-05"</span><span style=color:#eceff4>,
</span><span>      </span><span style=color:#a3be8c>"end_date"</span><span style=color:#eceff4>: </span><span style=color:#a3be8c>"2023-12"
</span><span>    }
</span><span>  ]</span><span style=color:#eceff4>,
</span><span>  </span><span style=color:#a3be8c>"skills"</span><span style=color:#eceff4>: </span><span>[
</span><span>    </span><span style=color:#a3be8c>"Python"</span><span style=color:#eceff4>,
</span><span>    </span><span style=color:#a3be8c>"Rust"
</span><span>  ]
</span><span>}
</span></code></pre><p>Prompt transparency, immediate visual feedback and testability are all key features that make BAML a joy to work with.<blockquote class="callout tip"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M9.97308 18H11V13H13V18H14.0269C14.1589 16.7984 14.7721 15.8065 15.7676 14.7226C15.8797 14.6006 16.5988 13.8564 16.6841 13.7501C17.5318 12.6931 18 11.385 18 10C18 6.68629 15.3137 4 12 4C8.68629 4 6 6.68629 6 10C6 11.3843 6.46774 12.6917 7.31462 13.7484C7.40004 13.855 8.12081 14.6012 8.23154 14.7218C9.22766 15.8064 9.84103 16.7984 9.97308 18ZM10 20V21H14V20H10ZM5.75395 14.9992C4.65645 13.6297 4 11.8915 4 10C4 5.58172 7.58172 2 12 2C16.4183 2 20 5.58172 20 10C20 11.8925 19.3428 13.6315 18.2443 15.0014C17.624 15.7748 16 17 16 18.5V21C16 22.1046 15.1046 23 14 23H10C8.89543 23 8 22.1046 8 21V18.5C8 17 6.37458 15.7736 5.75395 14.9992Z" fill=currentColor></path></svg></div><div class=content><p>BAML brings a level of rigour normally seen in software engineering workflows to LLM orchestration and agentic workflows.</div></blockquote><h3 id=language-agnosticity>Language agnosticity<a aria-label="Anchor link for: language-agnosticity" class=zola-anchor href=#language-agnosticity>#</a></h3><p>BAML supports codegen for generating library code in several languages. As of writing this post, Python, TypeScript, and Ruby are supported but support for Go, Rust, PHP and many other languages is coming soon. The benefit of using a DSL to set the LLM-related logic is that it can be statically defined upfront while allowing developers in various parts of the organization (who may be using different languages) to know to look in just one place, <code>baml_src</code>, implement the library code in their language of choice, and then use the generated code in <code>baml_client</code> to power their application.<blockquote class="callout tip"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M9.97308 18H11V13H13V18H14.0269C14.1589 16.7984 14.7721 15.8065 15.7676 14.7226C15.8797 14.6006 16.5988 13.8564 16.6841 13.7501C17.5318 12.6931 18 11.385 18 10C18 6.68629 15.3137 4 12 4C8.68629 4 6 6.68629 6 10C6 11.3843 6.46774 12.6917 7.31462 13.7484C7.40004 13.855 8.12081 14.6012 8.23154 14.7218C9.22766 15.8064 9.84103 16.7984 9.97308 18ZM10 20V21H14V20H10ZM5.75395 14.9992C4.65645 13.6297 4 11.8915 4 10C4 5.58172 7.58172 2 12 2C16.4183 2 20 5.58172 20 10C20 11.8925 19.3428 13.6315 18.2443 15.0014C17.624 15.7748 16 17 16 18.5V21C16 22.1046 15.1046 23 14 23H10C8.89543 23 8 22.1046 8 21V18.5C8 17 6.37458 15.7736 5.75395 14.9992Z" fill=currentColor></path></svg></div><div class=content><p>Due to its design, BAML potentially allows a much larger pool of developers using different languages to experience the same productivity and feature richness in the LLM ecosystem that Python developers have come to expect.</div></blockquote><h3 id=lossless-compression-in-token-space>Lossless compression in token space<a aria-label="Anchor link for: lossless-compression-in-token-space" class=zola-anchor href=#lossless-compression-in-token-space>#</a></h3><p>With BAML as the interface between the developer and the LLM, we achieve a form of <strong>lossless compression</strong> in token space. When passing instructions to the prompt, you can allow BAML to generate a more concise string representation of the instructions, rather than passing stringified JSON, which has a lot more extraneous tokens like double quotes and curly braces. LLMs benefit from concise, high quality instructions in the prompt. BAML formulates the prompt with the fewest tokens possible without losing any schema information.<figure><img alt="JSON schema (left): 370 tokens vs. BAML-generated schema (right): 168 tokens" loading=lazy src=baml-4.png><figcaption>JSON schema (left): 370 tokens vs. BAML-generated schema (right): 168 tokens</figcaption></figure><p>As can be seen, the resume schema example above shows how a JSON schema that explains to an LLM what fields are required and what fields aren’t, is quite verbose and hard to understand (both for humans and LLMs). This can be translated into an equivalent BAML-generated schema that uses far fewer tokens.<p>Upon receiving the LLM’s output, the BAML parser then validates the output and handles any errors, while coercing the output to the appropriate types so that it can be safely handled by any downstream part of the workflow.<blockquote class="callout tip"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M9.97308 18H11V13H13V18H14.0269C14.1589 16.7984 14.7721 15.8065 15.7676 14.7226C15.8797 14.6006 16.5988 13.8564 16.6841 13.7501C17.5318 12.6931 18 11.385 18 10C18 6.68629 15.3137 4 12 4C8.68629 4 6 6.68629 6 10C6 11.3843 6.46774 12.6917 7.31462 13.7484C7.40004 13.855 8.12081 14.6012 8.23154 14.7218C9.22766 15.8064 9.84103 16.7984 9.97308 18ZM10 20V21H14V20H10ZM5.75395 14.9992C4.65645 13.6297 4 11.8915 4 10C4 5.58172 7.58172 2 12 2C16.4183 2 20 5.58172 20 10C20 11.8925 19.3428 13.6315 18.2443 15.0014C17.624 15.7748 16 17 16 18.5V21C16 22.1046 15.1046 23 14 23H10C8.89543 23 8 22.1046 8 21V18.5C8 17 6.37458 15.7736 5.75395 14.9992Z" fill=currentColor></path></svg></div><div class=content><p>BAML lets the LLM focus on what it does best: generating strings token by token. The LLM’s attention window is largely occupied with understanding the task, not the schema.</div></blockquote><h2 id=baml-s-philosophy>BAML’s philosophy<a aria-label="Anchor link for: baml-s-philosophy" class=zola-anchor href=#baml-s-philosophy>#</a></h2><p>Since its inception, BAML’s goal has been to deliver the best possible developer experience when working with LLMs. However, for larger adoption of AI solutions across enterprise organizations, the <em>time-to-value</em> proposition is also hugely important.<p>BAML addresses this as follows:<ul><li>LLMs are incredibly powerful and versatile at a variety of tasks<li>Use them early on at various points in the workflow<li>Quickly deliver testable, working PoCs that offer immediate value to stakeholders<li>Use software engineering best practices to ship to production, iteratively adding complexity as necessary<li>Test the domain and business logic end-to-end and gather usage/success metrics<li>Replace LLMs with specialized tools where necessary (avoiding premature optimization and over-engineering)</ul><p>Developers can <strong>ship fast</strong> and iterate quickly (thanks to BAML’s rapid built/test cycle). All this without restricting software engineers to specific languages like Python or TypeScript, while being 100% open source and transparent.<h2 id=but-wait-didn-t-we-say-agents>But wait, didn’t we say agents?<a aria-label="Anchor link for: but-wait-didn-t-we-say-agents" class=zola-anchor href=#but-wait-didn-t-we-say-agents>#</a></h2><p>This isn’t the end of the story. BAML is just getting started 🔥. It’s all well and good to take a cursory glance at BAML and say, “<em>it doesn’t support graph-based workflows yet</em>”, or “<em>it doesn’t have features A, B, C like frameworks X, Y, Z</em>”. What’s clear once you begin using BAML is that it has a <strong>broad surface area</strong>. What started off as a goal towards improving structured generation with LLMs has now evolved into a toolchain that will eventually support a variety of complex control flows with logic-based retries, and more.<p>With the history of the BAML team’s relentless focus toward developer experience, it’s not a stretch to expect some great UX features for building agentic workflows in the coming weeks and months.<h2 id=conclusions>Conclusions<a aria-label="Anchor link for: conclusions" class=zola-anchor href=#conclusions>#</a></h2><p>To summarize, BAML is particularly appealing to developers like me because it simultaneously innovates along multiple dimensions that improve productivity and make coding more fun:<ul><li>Right level of abstraction, making code and logic more transparent and accessible<li>Tight iterative feedback loop, making prompt engineering a breeze<li>Language agnosticity, allowing developers to use their language of choice<li>Lossless compression in token space, meaning cheaper, faster workflows</ul><p>For my part, I’m excited to join the journey and to use BAML in my own projects related to knowledge graph extraction, graph quality improvement, Graph RAG, agentic RAG, and more. I’ll be exploring a lot more of these ideas using tools like Kùzu and LanceDB in conjunction with BAML in the coming months, so stay tuned! 🚀<h2 id=learning-resources>Learning resources<a aria-label="Anchor link for: learning-resources" class=zola-anchor href=#learning-resources>#</a></h2><p>I highly recommend the following blog posts by the Boundary team building BAML to understand their vision, philosophy and implementation. Follow along their journey, as I’m doing!<ul><li><a rel="nofollow noreferrer" href=https://www.boundaryml.com/blog/type-definition-prompting-baml>Your prompts are using 4x more tokens than you need</a><li><a rel="nofollow noreferrer" href=https://www.boundaryml.com/blog/structured-output-from-llms>Every way to structured outputs from LLMs</a><li><a rel="nofollow noreferrer" href=https://www.boundaryml.com/blog/schema-aligned-parsing#sap>What is Schema-Aligned Parsing (SAP)?</a><li><a rel="nofollow noreferrer" href=https://www.boundaryml.com/blog/building-a-new-programming-language>Building a new programming language in 2024</a><li><a rel="nofollow noreferrer" href=https://docs.boundaryml.com/home/welcome>BAML documentation</a></ul><p>The Data Exchange Podcast by Ben Lorica with Vaibhav Gupta, the founder of Boundary, is another great resource to get started learning about BAML. Let’s get building!<div class=youtube><iframe allowfullscreen mozallowfullscreen src=https://www.youtube.com/embed/VBNNm4NyaPw webkitallowfullscreen></iframe></div><hr><div class=footnote-definition id=1><sup class=footnote-definition-label>1</sup><p><a rel="nofollow noreferrer" href=https://blog.langchain.dev/langchain-second-birthday/>LangChain’s second birthday</a>, LangChain blog</div><div class=footnote-definition id=2><sup class=footnote-definition-label>2</sup><p><a rel="nofollow noreferrer" href=https://venturebeat.com/programming-development/python-data-validator-pydantic-launches-model-agnostic-ai-agent-development-platform/>Python data validator Pydantic launches model-agnostic AI agent development platform</a>, Venture Beat</div><div class=footnote-definition id=3><sup class=footnote-definition-label>3</sup><p><a rel="nofollow noreferrer" href=https://dottxt-ai.github.io/outlines/latest/welcome/>Outlines</a>, a Python library that allows you to use LLMs with structured generation</div><div class=footnote-definition id=4><sup class=footnote-definition-label>4</sup><p><a rel="nofollow noreferrer" href=https://www.boundaryml.com/blog/sota-function-calling>Beating OpenAI’s structured outputs on cost, accuracy and speed</a></div></article><div class=giscus></div><script async crossorigin data-category=General data-category-id=DIC_kwDOKyWhTs4CbUSt data-emit-metadata=0 data-input-position=bottom data-lang=en data-loading=lazy data-mapping=pathname data-reactions-enabled=1 data-repo=thedataquarry/thedataquarry.github.io data-repo-id=R_kgDOKyWhTg data-strict=0 data-theme=preferred_color_scheme src=https://giscus.app/client.js></script></div><footer><div class=copyright><p>© 2024 Prashanth Rao</div><div class=credits>Powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>