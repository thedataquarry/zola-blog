<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Vector databases (4): Analyzing the trade-offs | The Data Quarry</title><meta content="Vector databases (4): Analyzing the trade-offs" property=og:title><meta content="Prashanth Rao" name=author><meta content=en_US property=og:locale><meta content="A deeper dive into some of the trade-offs involved when choosing a vector DB" name=description><meta content="A deeper dive into some of the trade-offs involved when choosing a vector DB" property=og:description><link href=https://thedataquarry.github.io/posts/vector-db-4/ rel=canonical><meta content=https://thedataquarry.github.io/posts/vector-db-4/ property=og:url><meta content="The Data Quarry" property=og:site_name><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=og:image><meta content=article property=og:type><meta content=2023-08-19T00:00:00+00:00 property=article:published_time><meta " content=summary_large_image name=twitter:card><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=twitter:image><meta content="Vector databases (4): Analyzing the trade-offs" property=twitter:title><meta content=@tech_optimist name=twitter:site><meta content="A deeper dive into some of the trade-offs involved when choosing a vector DB" name=description><title>Vector databases (4): Analyzing the trade-offs</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#376dd9;--primary-pale-color:#698bcf1c;--inline-code-color:#444;--text-color:#444;--text-pale-color:#545967;--bg-color:#fff;--highlight-mark-color:#5f75b045;--callout-note-color:#3e70d6;--callout-important-color:#7a46cd;--callout-warning-color:#d3822b;--callout-alert-color:#d43f3f;--callout-question-color:#3089b5;--callout-tip-color:#35a06b;--main-font:"IBMPlexSans",ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:"IBMPlexMono",ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:750px;--main-max-width:750px;--avatar-size:175px;--paragraph-font-size:16px;--paragraph-line-height:1.5em;--aside-font-size:16px;--img-border-radius:4px;--inline-code-border-radius:2px}body.dark{--primary-color:#689afd;--primary-pale-color:#93acdd1c;--inline-code-color:#d2d2d2;--text-color:#ddd;--text-pale-color:#a0a0a0;--bg-color:#202124;--highlight-mark-color:#5f75b045;--callout-note-color:#698bcf;--callout-important-color:#9374c5;--callout-warning-color:#c99054;--callout-alert-color:#d35757;--callout-question-color:#5091b2;--callout-tip-color:#3ea06f}</style><link href=/main.css rel=stylesheet><script async data-website-id=01228822-c189-4b8a-93ba-733d045bf346 src=https://analytics.eu.umami.is/script.js></script><body class=post><script>if(localStorage.getItem('theme')=='dark'){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a href=/>The Data Quarry</a><button aria-label="toggle expand" class=separator id=toggler>::</button><span class="wrap left fold">{</span><a href=/posts>blog</a><span class="wrap-separator fold">,</span><a class=fold href=/talks>talks</a><span class="wrap-separator fold">,</span><a class=fold href=/projects>projects</a><span class="wrap right fold">} ;</span></nav><div id=btns><a rel="noreferrer noopener" aria-label=GitHub href=https://github.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=Twitter href=https://twitter.com/tech_optimist target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Twitter</title><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=LinkedIn href=https://www.linkedin.com/in/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill=currentColor></path></svg> </a><a aria-label="Buy me a coffee" rel="noreferrer noopener" href=https://www.buymeacoffee.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Buy Me A Coffee</title><path d="M20.216 6.415l-.132-.666c-.119-.598-.388-1.163-1.001-1.379-.197-.069-.42-.098-.57-.241-.152-.143-.196-.366-.231-.572-.065-.378-.125-.756-.192-1.133-.057-.325-.102-.69-.25-.987-.195-.4-.597-.634-.996-.788a5.723 5.723 0 00-.626-.194c-1-.263-2.05-.36-3.077-.416a25.834 25.834 0 00-3.7.062c-.915.083-1.88.184-2.75.5-.318.116-.646.256-.888.501-.297.302-.393.77-.177 1.146.154.267.415.456.692.58.36.162.737.284 1.123.366 1.075.238 2.189.331 3.287.37 1.218.05 2.437.01 3.65-.118.299-.033.598-.073.896-.119.352-.054.578-.513.474-.834-.124-.383-.457-.531-.834-.473-.466.074-.96.108-1.382.146-1.177.08-2.358.082-3.536.006a22.228 22.228 0 01-1.157-.107c-.086-.01-.18-.025-.258-.036-.243-.036-.484-.08-.724-.13-.111-.027-.111-.185 0-.212h.005c.277-.06.557-.108.838-.147h.002c.131-.009.263-.032.394-.048a25.076 25.076 0 013.426-.12c.674.019 1.347.067 2.017.144l.228.031c.267.04.533.088.798.145.392.085.895.113 1.07.542.055.137.08.288.111.431l.319 1.484a.237.237 0 01-.199.284h-.003c-.037.006-.075.01-.112.015a36.704 36.704 0 01-4.743.295 37.059 37.059 0 01-4.699-.304c-.14-.017-.293-.042-.417-.06-.326-.048-.649-.108-.973-.161-.393-.065-.768-.032-1.123.161-.29.16-.527.404-.675.701-.154.316-.199.66-.267 1-.069.34-.176.707-.135 1.056.087.753.613 1.365 1.37 1.502a39.69 39.69 0 0011.343.376.483.483 0 01.535.53l-.071.697-1.018 9.907c-.041.41-.047.832-.125 1.237-.122.637-.553 1.028-1.182 1.171-.577.131-1.165.2-1.756.205-.656.004-1.31-.025-1.966-.022-.699.004-1.556-.06-2.095-.58-.475-.458-.54-1.174-.605-1.793l-.731-7.013-.322-3.094c-.037-.351-.286-.695-.678-.678-.336.015-.718.3-.678.679l.228 2.185.949 9.112c.147 1.344 1.174 2.068 2.446 2.272.742.12 1.503.144 2.257.156.966.016 1.942.053 2.892-.122 1.408-.258 2.465-1.198 2.616-2.657.34-3.332.683-6.663 1.024-9.995l.215-2.087a.484.484 0 01.39-.426c.402-.078.787-.212 1.074-.518.455-.488.546-1.124.385-1.766zm-1.478.772c-.145.137-.363.201-.578.233-2.416.359-4.866.54-7.308.46-1.748-.06-3.477-.254-5.207-.498-.17-.024-.353-.055-.47-.18-.22-.236-.111-.71-.054-.995.052-.26.152-.609.463-.646.484-.057 1.046.148 1.526.22.577.088 1.156.159 1.737.212 2.48.226 5.002.19 7.472-.14.45-.06.899-.13 1.345-.21.399-.072.84-.206 1.08.206.166.281.188.657.162.974a.544.544 0 01-.169.364zm-6.159 3.9c-.862.37-1.84.788-3.109.788a5.884 5.884 0 01-1.569-.217l.877 9.004c.065.78.717 1.38 1.5 1.38 0 0 1.243.065 1.658.065.447 0 1.786-.065 1.786-.065.783 0 1.434-.6 1.499-1.38l.94-9.95a3.996 3.996 0 00-1.322-.238c-.826 0-1.491.284-2.26.613z" fill=currentColor></path></svg> </a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside class=blur><nav><ul><li><a class=h2 href=#choosing-the-right-vector-db-solution>Choosing the right vector DB solution</a> <ul><li><a class=h3 href=#1-on-prem-vs-cloud-hosting>1. On-prem vs. cloud hosting</a><li><a class=h3 href=#2-purpose-built-vendor-vs-incumbent-vendor>2. Purpose-built vendor vs. incumbent vendor</a><li><a class=h3 href=#3-insertion-speed-vs-query-speed>3. Insertion speed vs. query speed</a><li><a class=h3 href=#4-recall-vs-latency>4. Recall vs. latency</a><li><a class=h3 href=#5-in-memory-vs-on-disk-index-and-vector-storage>5. In-memory vs. on-disk index and vector storage</a><li><a class=h3 href=#6-sparse-vs-dense-vector-storage>6. Sparse vs. dense vector storage</a><li><a class=h3 href=#7-full-text-search-vs-vector-search-hybrid-strategy>7. Full-text search vs. vector search hybrid strategy</a><li><a class=h3 href=#8-filtering-strategy>8. Filtering strategy</a></ul><li><a class=h2 href=#conclusions>Conclusions</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Vector databases (4): Analyzing the trade-offs</h1><div id=post-info><div id=date><span id=publish>2023-08-19</span><span>Updated: <span id=updated>2023-11-29</span></span></div><div id=tags><a href=https://thedataquarry.github.io/tags/vector-db><span>#</span>vector-db</a></div></div><h2 id=choosing-the-right-vector-db-solution>Choosing the right vector DB solution<a aria-label="Anchor link for: choosing-the-right-vector-db-solution" class=zola-anchor href=#choosing-the-right-vector-db-solution>#</a></h2><p>Welcome back! In the <a href=../vector-db-3/>previous post</a> in this 4-part series, we looked at the different types of indexes typically used in vector DBs. However, indexing is just a small part of the bigger elephant in the room when it comes to vector databases. Recall that in <a href=../vector-db-2/#putting-it-all-together>part 2</a>, we described what a vector database <em>is</em>. To distinguish between the various vector DB offerings out there, we need to understand the relationships between the following components:<ul><li>Application layer, and where it sits<li>Data layer, and where it sits in relation to the <em>database</em> and the application layer<li>Indexing strategy, and how it relates to memory and CPU usage<li>Storage layer design<li>Scalability and cost considerations in relation to all these aspects</ul><p>Each of these components involve their own trade-offs. I’ve broken them down into the following categories – I’m sure there are more that I’ve missed, but this is a start. 🤓 Without further ado, let’s dive in.<h3 id=1-on-prem-vs-cloud-hosting>1. On-prem vs. cloud hosting<a aria-label="Anchor link for: 1-on-prem-vs-cloud-hosting" class=zola-anchor href=#1-on-prem-vs-cloud-hosting>#</a></h3><p>A lot of vendors stress on their “cloud-native” feature as though it’s the best thing since sliced bread, from a scalability perspective. However, in deciding what solution works best from the <em>cost</em> perspective, it’s important to consider how the hosting options are broken down in the first place. Embedded databases architectures are part of the options now! 🎉<p>Consider the following combinations:<ul><li>Cloud-native (managed) + client-server<li>On-prem (self-hosted) + embedded<li>Cloud-native (managed) + embedded</ul><p>It’s easier to compare these combinations visually.<figure><img loading=lazy src=vector-db-hosting-2.png></figure><p>It’s clear that the most common kind of database architecture is the client-server architecture, and that’s because it’s has been battle-tested by various other databases over many years. The embedded/serverless hosting model, are described below, is particularly interesting for the reasons described below.<h4 id=client-server-vs-embedded-architectures>Client-server vs. embedded architectures<a aria-label="Anchor link for: client-server-vs-embedded-architectures" class=zola-anchor href=#client-server-vs-embedded-architectures>#</a></h4><p>Ever since DuckDB monetized its open-source embedded SQL DB offering through MotherDuck<sup class=footnote-reference><a href=#1>1</a></sup>, it’s served as an inspiration to other upcoming database vendors. In the vector world, one vendor stands out on this front: <strong>LanceDB</strong><sup class=footnote-reference><a href=#2>2</a></sup>, a relatively new entrant – it utilizes an embedded, serverless design, and its open source version is ready-to-use and scalable from the get go.<figure><img loading=lazy src=vector-db-lancedb.png></figure><p>Chroma, another well-known offering, also offers an embedded option, but it’s purely in-memory and on a single machine, due to which they appear to be more inclined to move toward the tried-and-tested client-server model hosted on the cloud<sup class=footnote-reference><a href=#3>3</a></sup>. Zilliz, which wraps a serverless version of Milvus, also offers an embedded DB for its free version on the cloud<sup class=footnote-reference><a href=#12>4</a></sup>. However, LanceDB is the only one that’s built as embedded serverless for all modes of operation.<p>For organizations that hold a lot of data spread across on-premise and cloud storage, embedded/serverless vector DBs offer a lot of freedom and flexibility to developers when connecting the data layers to the application layer (where users interact with the data), especially when privacy and security are a concern.<p>Despite a lot of vendors offering fully-managed, cloud-hosted solutions with secure endpoints deployed within virtual private clouds (VPCs), organizations could be hesitant to move all of their data and vector compute to the cloud. This is especially true for large organizations with highly sensitive data that have to interface with legacy systems that are not cloud-native. In these cases, it’s often easier to deploy an embedded vector database like LanceDB on-prem, and very easily connect it to the application layer.<h4 id=cost-considerations>Cost considerations<a aria-label="Anchor link for: cost-considerations" class=zola-anchor href=#cost-considerations>#</a></h4><p>Databases that offer cloud-native, managed solutions typically charge based on the amount of data stored and the number of queries made. This is a great model for organizations that have a lot of data, but don’t want to pay for the infrastructure to transport and store it. However, for organizations that have a lot of data, but already have established in-house data infrastructure teams ready to support on-prem or embedded hosting, sending data to the cloud, both for indexing and for querying, makes almost no sense.<p>While it’s hard to define a “critical mass” of an organization’s size at which point this distinction can be made, it’s important to know that the standard client-server database architectures deployed on the cloud that we have been used to for many years, are <strong>NOT</strong> the only option in 2023.<p>Questions to ask:<ul><li>Is my dataset growing fast enough that I need to scale it elastically, on demand, on the cloud?<li>Do I have an established enough data infrastructure team to support on-prem/embedded hosting?<li>Is my dataset sensitive enough that I need to keep it on-prem?<li>What might be the cost considerations of paying per query and per GB of data stored for a fully-managed cloud-hosted solution?</ul><h3 id=2-purpose-built-vendor-vs-incumbent-vendor>2. Purpose-built vendor vs. incumbent vendor<a aria-label="Anchor link for: 2-purpose-built-vendor-vs-incumbent-vendor" class=zola-anchor href=#2-purpose-built-vendor-vs-incumbent-vendor>#</a></h3><p>A lot of organizations are already invested in incumbent solutions (like Elasticsearch, Meilisearch, MongoDB, etc.). These solutions are not purpose-built for vector search, but they do add some vector search capabilities on top of their existing ones. If you’re looking to add vector/semantic search capabilities on top of an existing application, it makes sense to first at least try out the vector search capabilities of your existing database, and consider the cost implications of these solutions before looking outward.<p>However, the vector search capabilities offered by incumbent vendors come with certain caveats. Elasticsearch, for example, has a lot of additional constraints placed on what sorts of clients can use their vector search offering, and may require specialized deployments and licensing terms to access their full suite of tools. MongoDB offers vector search <em>only</em> for their Atlas cloud deployment, and not on their self-hosted solution. These are important considerations!<p>The other key fact to keep in mind is this: purpose-built vector DB vendors like Qdrant, Weaviate and LanceDB and many others, have spent thousands of man-hours optimizing their storage, indexing and querying strategies for vector search. They have also had the luxury to build their systems from the ground up, often in modern programming languages like Go or Rust, and as such, are designed for massive scalability and performance. Pretty much every purpose-built vector database solution offers a Python or Javascript client, so it’s easy to at least begin testing them on your own data to make a more informed decision.<p>Questions to ask:<ul><li>Do I already have an existing database that I can use for vector search?<li>Can I run a simple benchmark on my own data to compare the performance of an incumbent solution vs. a purpose-built solution?<li>Will this solution scale as my data grows?</ul><p>In many cases, the reality is that an existing solution that adds vector search is not going to be as performant as a purpose-built vector database. This can be seen very clearly in this benchmark study<sup class=footnote-reference><a href=#4>5</a></sup> comparing pgvector/PostgreSQL vs. Qdrant – Spoiler: pgvector is <em>much</em> slower than Qdrant on all counts, and offers far fewer optimizations under the hood.<h3 id=3-insertion-speed-vs-query-speed>3. Insertion speed vs. query speed<a aria-label="Anchor link for: 3-insertion-speed-vs-query-speed" class=zola-anchor href=#3-insertion-speed-vs-query-speed>#</a></h3><p>Certain vendors like Milvus/Zilliz, have been around for long enough that they are looking at humongous-scale streaming use cases that require vectors. Inserting, and then indexing thousands of vectors per minute may be essential in real-time scenarios like video surveillance or financial transaction tracking, and Milvus/Zilliz are capable of throwing the kitchen sink at this problem<sup class=footnote-reference><a href=#5>6</a></sup>.<p>However, is insertion speed that important to the majority of use cases? For most organizations, querying speed is more important. This is because insertion and indexing are typically done infrequently, but the data might be queried much more frequently, typically in real-time at scale via user interfaces.<p>Qdrant, an open-source, purpose-built vector DB written in Rust 🦀, optimizes exactly for this use case. Because it’s written in Rust, it performs admirably fast in indexing as well, but, as shown in the Qdrant demo on their own documentation page, semantic search-as-you-type, returning vector search solutions that produce relevant results in just a few milliseconds, is actually achievable in a live setting. 🤯<figure><img alt="Search as you type, <a href='https://twitter.com/qdrant_engine/status/1691054852913418241'>Qdrant Twitter</a>" loading=lazy src=qdrant-search-as-you-type.png><figcaption>Search as you type, <a href=https://twitter.com/qdrant_engine/status/1691054852913418241>Qdrant Twitter</a></figcaption></figure><p>Questions to ask:<ul><li>How often am I going to be inserting (and indexing) a large number of vectors?<li>Do I meet the latency requirements of my application at query time?</ul><h3 id=4-recall-vs-latency>4. Recall vs. latency<a aria-label="Anchor link for: 4-recall-vs-latency" class=zola-anchor href=#4-recall-vs-latency>#</a></h3><p>Recall is the percentage of relevant results returned by a query, and latency is the time taken to return the results. Different database vendor make different trade-offs when it comes to optimizing for recall vs. latency.<p>As summarized in <a href=../vector-db-3/>part 3</a> of this series:<ul><li>A <code>Flat</code> index is one that stores vectors in their unmodified form, and is used for exact kNN search. It is the most accurate, but also the slowest.<li><code>IVF-Flat</code> indexes use inverted file indexes to rapidly narrow down on the search space, which are much faster than brute force search, but they sacrifice some accuracy in the form of recall<li><code>IVF-PQ</code> uses IVF in combination with Product Quantization to compress the vectors, reducing the memory footprint and speeding up search, while being better in recall than a pure <code>PQ</code> index<li><code>HNSW</code> is by far the most popular index, and can be combined with Product Quantization, in the form of <code>HNSW-PQ</code>, to produce better recall while improving memory efficiency compared to <code>IVF-PQ</code><li><code>Vamana</code> is a relatively new index, designed and optimized for on-disk performance – it offers the promise of storing larger-than-memory vector data while performing as well, and as fast, as <code>HNSW</code> <ul><li>However, it’s still early days and not many databases have made the leap towards implementing it due to the challenges of on-disk performance</ul></ul><p>Keeping all these trade-offs in mind, we can reference the image from earlier in this series.<figure><img loading=lazy src=../vector-db-1/vector-db-indexes.png></figure><p>IVF-PQ makes sense for early-stage use cases and POCs where perfect relevance isn’t critical to the success of the application. However, for better quality and relevance, most purpose-built vendors use the HNSW index. Of late, vendors like Weaviate and Qdrant have begun combining product quantization (PQ) with HNSW to improve memory efficiency for really large datasets.<p>Questions to ask:<ul><li>How critical is search recall for my use case? <ul><li>Typically, a benchmark study on your own data and queries will help you answer this question.</ul><li>How important is latency in my use case? <ul><li>If the dataset really large, then it might make sense to use a product-quantized index like IVF-PQ or HNSW-PQ to reduce memory footprint.</ul></ul><h3 id=5-in-memory-vs-on-disk-index-and-vector-storage>5. In-memory vs. on-disk index and vector storage<a aria-label="Anchor link for: 5-in-memory-vs-on-disk-index-and-vector-storage" class=zola-anchor href=#5-in-memory-vs-on-disk-index-and-vector-storage>#</a></h3><p>Databases like Redis are completely in-memory, and are blazing fast. However, it’s very plausible that your use case requires storing enough vectors that are larger than memory. A combination of tricks is required to scale vector storage to very large data (hundreds of millions, or even billions of vectors) while also maintaining ANN search speed. Databases like Qdrant and Weaviate provide the option of using memory-mapped files for vectors that utilize the page cache’s virtual address space on disk, avoiding loading the entire data into RAM. This helps maintain almost the speed of in-memory databases without actually persisting the data to the disk.<p>From an indexing perspective, HNSW is known for being the most memory-hungry, and as vector datasets get larger and larger, the natural question that arises is, how well does it scale to out-of-memory indexes? Memory needs can be reduced by combining PQ with HNSW, as described in the previous section. Vamana, a relatively newer index, which is part of the DiskANN algorithm, is among the most promising methods out there (currently available only in LanceDB<sup class=footnote-reference><a href=#6>7</a></sup> and Milvus<sup class=footnote-reference><a href=#7>8</a></sup>), and is claimed to perform on par with HNSW while scaling to larger-than-memory indexes <strong>purely on-disk</strong>.<blockquote class="callout note"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22ZM12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20ZM11 7H13V9H11V7ZM11 11H13V17H11V11Z" fill=currentColor></path></svg></div><div class=content><p><strong>Note</strong><p>Of all the database vendors out there, <a rel="nofollow noreferrer" href=https://lancedb.github.io/lancedb/ann_indexes/>LanceDB</a> stands out in this aspect, because it’s the <strong>only</strong> one in which <em>all</em> supported indexes are disk-based. When responding to a vector query, only the relevant pages from the index file are loaded from disk and cached in memory. It’s able to do this largely because LanceDB builds on top of <a rel="nofollow noreferrer" href=https://github.com/lancedb/lance>Lance</a>, a new open source columnar vector store that’s optimized for fast on-disk vector retrieval.</div></blockquote><p>Questions to ask:<ul><li>If my dataset is really large (>10M vectors), how can I reduce memory consumption? In these cases, reducing the dimensionality of the vectors being stored, tuning the maximum number of graph connections (if using HNSW), or adding product quantization (if your DB supports it) can help.<li>Does my database have an option to store vectors on disk, and if so, how does it affect query speed? As always, test on your own data and use case!</ul><h3 id=6-sparse-vs-dense-vector-storage>6. Sparse vs. dense vector storage<a aria-label="Anchor link for: 6-sparse-vs-dense-vector-storage" class=zola-anchor href=#6-sparse-vs-dense-vector-storage>#</a></h3><p>The vector embeddings generated by <code>sentence-transformers</code> or similar models are dense, meaning that they are composed entirely of non-zero floats. However, it’s possible to also use sparse vectors that compute the relative word frequencies per document, in which most of the vector values are zero. Sparse vectors are typically generated by algorithms like BM25 and SPLADE (Sparse Lexical AnD Expansion). Elasticsearch offers its own proprietary pre-trained sparse model for English, ELSER (Elasticsearch Learned Sparse Encoder), that has roughly 30,000 dimensions, but because it’s sparse, it’s far cheaper to compute and store than an equivalent-length dense vector.<p>With the advent of <code>sentence-transformers</code> and many other transformer models, generating dense vectors from documents has never been easier (or more affordable). The main benefit of dense vectors are that they compress the semantics of language much better than sparse vectors do, due to the underlying embeddings that come from transformers — however, they are more expensive at indexing time, which is definitely a consideration when dealing with datasets of the order of 100M vectors.<p>Questions to ask:<ul><li>How important is semantic search in my use case? If it’s very important, then dense vectors are absolutely the way to go<li>How important is latency and speed of indexing? If these are very important constraints, then sparse vectors might be worth looking at (though they will never perform as well as dense vectors from a semantic search standpoint)</ul><h3 id=7-full-text-search-vs-vector-search-hybrid-strategy>7. Full-text search vs. vector search hybrid strategy<a aria-label="Anchor link for: 7-full-text-search-vs-vector-search-hybrid-strategy" class=zola-anchor href=#7-full-text-search-vs-vector-search-hybrid-strategy>#</a></h3><p>It’s well-known that vector search is no panacea. I’ll again point the reader to Colin Harman’s excellent blog post<sup class=footnote-reference><a href=#8>9</a></sup>, where he states this key fact:<blockquote><p>The unfortunate truth is that, for enterprise-level applications, plain vector search is frequently the wrong solution. Many practitioners understand this and often use vector search as a part of their information retrieval system.</blockquote><p>The main reason for this is that vector search can rank semantically similar results above exact matches, and in many use cases, we may want exact matches to score higher than a semantically similar match. Also, as languages and terminologies evolve, the “meaning” of certain terms shift, and these may not be captured well enough by the underlying embeddings. Re-ranking the results from a vector search, typically using scores from a full-text keyword-based search, or by combining indexing strategies (like using a combination of an inverted index and a vector index) can help improve the quality of results. Vespa, a hybrid search engine, that offers an <code>HNSW-IVF</code> index, is a good example of this<sup class=footnote-reference><a href=#9>10</a></sup>.<p>A few techniques to handle this trade-off via re-ranking can be considered, and are described below.<ul><li><p><strong>Naive fallback approach</strong>: This approach uses a “fallback” strategy, where you start with keyword search-as-you-type, using a solution that implements BM25, like <a rel="nofollow noreferrer" href=https://www.meilisearch.com/>Meilisearch</a> or Elasticsearch, with the higher-latency vector search being used in a linear combination with the keyword search results. This is the simplest approach, and doesn’t always yield the best results in terms of relevance.</p><li><p><strong>Reciprocal Rank Fusion (RRF)</strong>: This approach sums up the reciprocal ranks obtained from sparse and dense vectors to provide a fused ranking score. Databases like Elasticsearch and Weaviate<sup class=footnote-reference><a href=#10>11</a></sup> offer hybrid search using one of many RRF methods.</p><li><p><strong>Cross-encoder reranking</strong>: This is the most advanced method for hybrid score re-ranking. It fuses the sparse/dense ranking scores using a neural bi-encoder with a cross-encoding loss function, which typically produces the best results, at the expense of higher latency due to re-ranking via a more expensive approach. These solutions aren’t typically offered directly in vector databases – custom search engines need to be built downstream of the vector DB to perform the re-ranking. Nils Reimers, creator of the <code>sentence-transformers</code> library, who is now at Cohere.ai, describes exactly such solutions in an episode of the Weaviate podcast linked below. 😄</p></ul><div class=youtube><iframe allowfullscreen mozallowfullscreen src=https://www.youtube.com/embed/KITxQzV97jw webkitallowfullscreen></iframe></div><p>Questions to ask:<ul><li>Does my database of choice implement a solid hybrid search strategy? Or will I have to hand-craft a solution downstream of the vector database?<li>How is search latency affected by the re-ranking strategy?<li>How much latency can I afford to add to my search results by improving accuracy (for e.g., through cross-encoder re-ranking)?</ul><h3 id=8-filtering-strategy>8. Filtering strategy<a aria-label="Anchor link for: 8-filtering-strategy" class=zola-anchor href=#8-filtering-strategy>#</a></h3><p>In search, real-world queries are rarely simple textual queries that ask for specific keywords. They typically involve filtering on other metadata attributes. Consider a retail example involving a clothing search (pants, jeans, etc.) of a particular size – the way these filters are applied to the search result can have a huge impact on the quality of results.<ul><li><p><strong>Pre-filtered search</strong>: This approach involves naïvely applying the size filter before performing vector search. Although this sounds like a natural thing to attempt, this can lead to a collapse of the HSNW graph into disjoint connected components (as per percolation theory<sup class=footnote-reference><a href=#11>12</a></sup>, which defines a threshold at which this happens). As an example, if the user was looking for “jeans” of size 28 which are not in the search catalog, the right way to go would be to at least show related items (like “pants”) of the similar size. But, because pre-filtering aggressively prunes all the terms out of the search that don’t meet the size criteria, the full graph isn’t traversed and we miss out on relevant results.</p><li><p><strong>Post-filtered search</strong>: This approach returns <code>top-k</code> nearest neighbours to the query vector (“jeans”) and filters them down based on matching sizes after all “jeans” results are retrieved. However, this also has a problem – we don’t always obtain the same number of results for every query, and if the filtered attribute, i.e., size 28, is a very small fraction of the entire dataset, we may obtain almost no results at all!</p><li><p><strong>Custom-filtered search</strong>: Numerous databases apply methods that are in between pre/post-filtering — Weaviate, for example, stores inverted index shards alongside the HNSW index shards, and uses the inverted index to pre-filter much more effectively — the “allow list” obtained from the inverted index, which is a quite large list, is then much more effectively searched via the HNSW index, which considers semantics. Qdrant uses its own filtering method in between pre/post-filtering that ensures a connected HNSW graph under a wide range of conditions by forming edges between categories at index time.</p></ul><blockquote class="callout warning"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M12.865 3.00017L22.3912 19.5002C22.6674 19.9785 22.5035 20.5901 22.0252 20.8662C21.8732 20.954 21.7008 21.0002 21.5252 21.0002H2.47266C1.92037 21.0002 1.47266 20.5525 1.47266 20.0002C1.47266 19.8246 1.51886 19.6522 1.60663 19.5002L11.1329 3.00017C11.4091 2.52187 12.0206 2.358 12.4989 2.63414C12.651 2.72191 12.7772 2.84815 12.865 3.00017ZM4.20471 19.0002H19.7932L11.9989 5.50017L4.20471 19.0002ZM10.9989 16.0002H12.9989V18.0002H10.9989V16.0002ZM10.9989 9.00017H12.9989V14.0002H10.9989V9.00017Z" fill=currentColor></path></svg></div><div class=content><p><strong>Note</strong><p>There is no “one-size-fits-all” solution for filtering results in vector search. Multiple mitigation strategies exist, such as building an additional IVF index to use keywords to assist in the search, as in <a rel="nofollow noreferrer" href=https://weaviate.io/blog/hybrid-search-explained>Weaviate</a>, or creating additional HNSW graph connections between <em>categories</em>, which helps consider all buckets in the search during filtering, as in <a rel="nofollow noreferrer" href=https://qdrant.tech/articles/filtrable-hnsw/>Qdrant</a>. This area is continuously evolving, and highly relevant search & retrieval is an incredibly hard problem to solve, due to these numerous trade-offs.</div></blockquote><p>Questions to ask:<ul><li>How does my database of choice handle pre/post filtering?<li>For the classes of queries that I will be performing, how well does the pre/post filtering strategy work on my data?</ul><h2 id=conclusions>Conclusions<a aria-label="Anchor link for: conclusions" class=zola-anchor href=#conclusions>#</a></h2><p>Whew! This has been a long road toward understanding the internals of vector databases. I’ve been thinking deeply about this topic for most of 2023, and have been down many rabbit-holes, having had a ton of interesting conversations with developers, CEOs and other folks experimenting with these tools and technologies. 🤓<p>As vector DBs continue to evolve in this rapidly changing space, I think the key takeaway from this series, at least for me, is that there is no one-size-fits-all solution. The best way to choose a vector database stack is to first understand the requirements and constraints of your use case, and then test out the different solutions on your own data.<p>In my experience, purpose-built solutions are the superior option, because they have a wider of suite of functions, are able to implement cutting-edge solutions due to starting from a clean-slate, and they also contain a host of optimizations that incumbent vendors just can’t prioritize for.<p>Looking ahead, I’m particularly excited about two databases built in Rust 🦀, namely, Qdrant and LanceDB, which will be my go-to solutions for any POCs and experiments. They’re both innovating at a new level in very different ways, and most importantly, they both have a <strong>developer-first</strong> philosophy, with a rapidly growing community. Regardless of which vendor you fancy, I urge you to join me and many others in exploring these tools further! 🤓<p>If you like listening to podcasts, I speak about these topics at length in the <a rel="nofollow noreferrer" href=https://practicalai.fm/234>Practical AI Podcast</a>. Happy learning, and keep coding! 🚀<p><strong>Other posts in this series</strong><ul><li><a href=../vector-db-1/>Vector databases (Part 1): What makes each one different?</a><li><a href=../vector-db-2/>Vector databases (Part 2): Understanding their internals</a><li><a href=../vector-db-3/>Vector databases (Part 3): Not all indexes are created equal</a></ul><hr><div class=footnote-definition id=1><sup class=footnote-definition-label>1</sup><p>Teach your DuckDB to fly, <a rel="nofollow noreferrer" href=https://motherduck.com/>MotherDuck</a></div><div class=footnote-definition id=2><sup class=footnote-definition-label>2</sup><p>Developer-friendly, serverless vector database: <a rel="nofollow noreferrer" href=https://lancedb.com/>LanceDB</a></div><div class=footnote-definition id=3><sup class=footnote-definition-label>3</sup><p>Deployment: <a rel="nofollow noreferrer" href=https://docs.trychroma.com/deployment>Chroma docs</a></div><div class=footnote-definition id=12><sup class=footnote-definition-label>4</sup><p>Zilliz pricing, <a rel="nofollow noreferrer" href=https://zilliz.com/pricing>zilliz.com</a></div><div class=footnote-definition id=4><sup class=footnote-definition-label>5</sup><p>pgvector vs. Qdrant, Nirant Kasliwal’s <a rel="nofollow noreferrer" href=https://nirantk.com/writing/pgvector-vs-qdrant/>blog</a></div><div class=footnote-definition id=5><sup class=footnote-definition-label>6</sup><p>Managing data in massive-scale vector search engine, Milvus <a rel="nofollow noreferrer" href=https://milvus.io/blog/2019-11-08-data-management.md>blog</a></div><div class=footnote-definition id=6><sup class=footnote-definition-label>7</sup><p>ANN indexes FAQ, LanceDB <a rel="nofollow noreferrer" href=https://lancedb.github.io/lancedb/ann_indexes/>docs</a></div><div class=footnote-definition id=7><sup class=footnote-definition-label>8</sup><p>On-disk index, Milvus <a rel="nofollow noreferrer" href=https://milvus.io/docs/disk_index.md>docs</a></div><div class=footnote-definition id=8><sup class=footnote-definition-label>9</sup><p>Beware tunnel vision in AI retrieval: Colin Harman, <a rel="nofollow noreferrer" href=https://colinharman.substack.com/p/beware-tunnel-vision-in-ai-retrieval>Substack</a></div><div class=footnote-definition id=9><sup class=footnote-definition-label>10</sup><p>Billion-scale vector search using hybrid HNSW, by <a rel="nofollow noreferrer" href=https://medium.com/vespa/billion-scale-vector-search-using-hybrid-hnsw-if-96d7058037d3>Jo Kristian Bergum</a></div><div class=footnote-definition id=10><sup class=footnote-definition-label>11</sup><p>Hybrid search explained, Weaviate blog, by <a rel="nofollow noreferrer" href=https://weaviate.io/blog/hybrid-search-explained>Erika Cardenas</a></div><div class=footnote-definition id=11><sup class=footnote-definition-label>12</sup><p>Filterable HNSW, Qdrant <a rel="nofollow noreferrer" href=https://qdrant.tech/articles/filtrable-hnsw/>blog</a></div></article><div class=giscus></div><script async crossorigin data-category=General data-category-id=DIC_kwDOKyWhTs4CbUSt data-emit-metadata=0 data-input-position=bottom data-lang=en data-loading=lazy data-mapping=pathname data-reactions-enabled=1 data-repo=thedataquarry/thedataquarry.github.io data-repo-id=R_kgDOKyWhTg data-strict=0 data-theme=preferred_color_scheme src=https://giscus.app/client.js></script></div><footer><div class=copyright><p>© 2024 Prashanth Rao</div><div class=credits>Powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>