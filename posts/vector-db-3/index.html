<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Vector databases (3): Not all indexes are created equal | The Data Quarry</title><meta content="Vector databases (3): Not all indexes are created equal" property=og:title><meta content="Prashanth Rao" name=author><meta content=en_US property=og:locale><meta content="Understanding Flat, Annoy, IVF, HNSW and Vamana vector indexes in vector databases" name=description><meta content="Understanding Flat, Annoy, IVF, HNSW and Vamana vector indexes in vector databases" property=og:description><link href=https://thedataquarry.github.io/posts/vector-db-3/ rel=canonical><meta content=https://thedataquarry.github.io/posts/vector-db-3/ property=og:url><meta content="The Data Quarry" property=og:site_name><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=og:image><meta content=article property=og:type><meta content=2023-07-24T00:00:00+00:00 property=article:published_time><meta " content=summary_large_image name=twitter:card><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=twitter:image><meta content="Vector databases (3): Not all indexes are created equal" property=twitter:title><meta content=@tech_optimist name=twitter:site><meta content="Understanding Flat, Annoy, IVF, HNSW and Vamana vector indexes in vector databases" name=description><title>Vector databases (3): Not all indexes are created equal</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#376dd9;--primary-pale-color:#698bcf1c;--inline-code-color:#444;--text-color:#444;--text-pale-color:#545967;--bg-color:#fff;--highlight-mark-color:#5f75b045;--callout-note-color:#3e70d6;--callout-important-color:#7a46cd;--callout-warning-color:#d3822b;--callout-alert-color:#d43f3f;--callout-question-color:#3089b5;--callout-tip-color:#35a06b;--main-font:"IBMPlexSans",ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:"IBMPlexMono",ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:750px;--main-max-width:750px;--avatar-size:175px;--paragraph-font-size:16px;--paragraph-line-height:1.5em;--aside-font-size:16px;--img-border-radius:4px;--inline-code-border-radius:2px}body.dark{--primary-color:#689afd;--primary-pale-color:#93acdd1c;--inline-code-color:#d2d2d2;--text-color:#ddd;--text-pale-color:#a0a0a0;--bg-color:#202124;--highlight-mark-color:#5f75b045;--callout-note-color:#698bcf;--callout-important-color:#9374c5;--callout-warning-color:#c99054;--callout-alert-color:#d35757;--callout-question-color:#5091b2;--callout-tip-color:#3ea06f}</style><link href=/main.css rel=stylesheet><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI rel=stylesheet><script crossorigin defer integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js></script><script crossorigin defer integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false})})</script><script async data-website-id=01228822-c189-4b8a-93ba-733d045bf346 src=https://analytics.eu.umami.is/script.js></script><body class=post><script>if(localStorage.getItem('theme')=='dark'){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a href=/>The Data Quarry</a><button aria-label="toggle expand" class=separator id=toggler>::</button><span class="wrap left fold">{</span><a href=/posts>blog</a><span class="wrap-separator fold">,</span><a class=fold href=/talks>talks</a><span class="wrap-separator fold">,</span><a class=fold href=/projects>projects</a><span class="wrap right fold">} ;</span></nav><div id=btns><a rel="noreferrer noopener" aria-label=GitHub href=https://github.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=Twitter href=https://twitter.com/tech_optimist target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Twitter</title><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=LinkedIn href=https://www.linkedin.com/in/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill=currentColor></path></svg> </a><a aria-label="Buy me a coffee" rel="noreferrer noopener" href=https://www.buymeacoffee.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Buy Me A Coffee</title><path d="M20.216 6.415l-.132-.666c-.119-.598-.388-1.163-1.001-1.379-.197-.069-.42-.098-.57-.241-.152-.143-.196-.366-.231-.572-.065-.378-.125-.756-.192-1.133-.057-.325-.102-.69-.25-.987-.195-.4-.597-.634-.996-.788a5.723 5.723 0 00-.626-.194c-1-.263-2.05-.36-3.077-.416a25.834 25.834 0 00-3.7.062c-.915.083-1.88.184-2.75.5-.318.116-.646.256-.888.501-.297.302-.393.77-.177 1.146.154.267.415.456.692.58.36.162.737.284 1.123.366 1.075.238 2.189.331 3.287.37 1.218.05 2.437.01 3.65-.118.299-.033.598-.073.896-.119.352-.054.578-.513.474-.834-.124-.383-.457-.531-.834-.473-.466.074-.96.108-1.382.146-1.177.08-2.358.082-3.536.006a22.228 22.228 0 01-1.157-.107c-.086-.01-.18-.025-.258-.036-.243-.036-.484-.08-.724-.13-.111-.027-.111-.185 0-.212h.005c.277-.06.557-.108.838-.147h.002c.131-.009.263-.032.394-.048a25.076 25.076 0 013.426-.12c.674.019 1.347.067 2.017.144l.228.031c.267.04.533.088.798.145.392.085.895.113 1.07.542.055.137.08.288.111.431l.319 1.484a.237.237 0 01-.199.284h-.003c-.037.006-.075.01-.112.015a36.704 36.704 0 01-4.743.295 37.059 37.059 0 01-4.699-.304c-.14-.017-.293-.042-.417-.06-.326-.048-.649-.108-.973-.161-.393-.065-.768-.032-1.123.161-.29.16-.527.404-.675.701-.154.316-.199.66-.267 1-.069.34-.176.707-.135 1.056.087.753.613 1.365 1.37 1.502a39.69 39.69 0 0011.343.376.483.483 0 01.535.53l-.071.697-1.018 9.907c-.041.41-.047.832-.125 1.237-.122.637-.553 1.028-1.182 1.171-.577.131-1.165.2-1.756.205-.656.004-1.31-.025-1.966-.022-.699.004-1.556-.06-2.095-.58-.475-.458-.54-1.174-.605-1.793l-.731-7.013-.322-3.094c-.037-.351-.286-.695-.678-.678-.336.015-.718.3-.678.679l.228 2.185.949 9.112c.147 1.344 1.174 2.068 2.446 2.272.742.12 1.503.144 2.257.156.966.016 1.942.053 2.892-.122 1.408-.258 2.465-1.198 2.616-2.657.34-3.332.683-6.663 1.024-9.995l.215-2.087a.484.484 0 01.39-.426c.402-.078.787-.212 1.074-.518.455-.488.546-1.124.385-1.766zm-1.478.772c-.145.137-.363.201-.578.233-2.416.359-4.866.54-7.308.46-1.748-.06-3.477-.254-5.207-.498-.17-.024-.353-.055-.47-.18-.22-.236-.111-.71-.054-.995.052-.26.152-.609.463-.646.484-.057 1.046.148 1.526.22.577.088 1.156.159 1.737.212 2.48.226 5.002.19 7.472-.14.45-.06.899-.13 1.345-.21.399-.072.84-.206 1.08.206.166.281.188.657.162.974a.544.544 0 01-.169.364zm-6.159 3.9c-.862.37-1.84.788-3.109.788a5.884 5.884 0 01-1.569-.217l.877 9.004c.065.78.717 1.38 1.5 1.38 0 0 1.243.065 1.658.065.447 0 1.786-.065 1.786-.065.783 0 1.434-.6 1.499-1.38l.94-9.95a3.996 3.996 0 00-1.322-.238c-.826 0-1.491.284-2.26.613z" fill=currentColor></path></svg> </a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside class=blur><nav><ul><li><a class=h2 href=#organizing-vector-indexes>Organizing vector indexes</a><li><a class=h2 href=#level-1-data-structures>Level 1: Data structures</a> <ul><li><a class=h3 href=#hash-based-index>Hash-based index</a><li><a class=h3 href=#tree-based-index>Tree-based index</a><li><a class=h3 href=#graph-based-index>Graph-based index</a><li><a class=h3 href=#inverted-file-index>Inverted file index</a></ul><li><a class=h2 href=#level-2-compression>Level 2: Compression</a> <ul><li><a class=h3 href=#flat-indexes>Flat indexes</a><li><a class=h3 href=#quantized-indexes>Quantized indexes</a></ul><li><a class=h2 href=#popular-indexes>Popular indexes</a> <ul><li><a class=h3 href=#ivf-pq>IVF-PQ</a><li><a class=h3 href=#hnsw>HNSW</a><li><a class=h3 href=#vamana>Vamana</a></ul><li><a class=h2 href=#indexes-in-popular-vector-dbs>Indexes in popular vector DBs</a><li><a class=h2 href=#conclusions>Conclusions</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Vector databases (3): Not all indexes are created equal</h1><div id=post-info><div id=date><span id=publish>2023-07-24</span><span>Updated: <span id=updated>2023-11-29</span></span></div><div id=tags><a href=https://thedataquarry.github.io/tags/vector-db><span>#</span>vector-db</a></div></div><h2 id=organizing-vector-indexes>Organizing vector indexes<a aria-label="Anchor link for: organizing-vector-indexes" class=zola-anchor href=#organizing-vector-indexes>#</a></h2><p>This is the third post in a series on vector databases. <a href=../vector-db-1/>Part 1</a> compared the offerings of various DB vendors and how they are different at a high level, while <a href=../vector-db-2/>Part 2</a> focused on the basics of what vector DBs are and what they do.<p>You may have already come across the excellent post “<em>Not all vector databases are made equal</em>”<sup class=footnote-reference><a href=#1>1</a></sup> by Dmitry Kan, which covered the differences between various vector DBs in the market back in 2021. The vector DB landscape has been continuously evolving since then, and because each one is quite different from the others, I thought it made sense to do a deeper dive into indexing methods in this post.<p>Assuming that it’s amply clear <a href=../vector-db-2/#putting-it-all-together>what a vector database <em>is</em></a>, it’s worth taking a step back to wonder, how does it all scale so wonderfully to be able to search millions, billions, or even trillions<sup class=footnote-reference><a href=#2>2</a></sup> of vectors? As we know, the primary aim of a vector database is to provide a fast and efficient means to store and <em>semantically</em> query data, in a way that the vector data type is a first-class citizen. The similarity between two vectors is gauged by distance metrics like cosine distance or the dot product.<p>When working with vector databases, it’s important to distinguish between the <em>search algorithm</em>, and the underlying <em>index</em> on which the Approximate Nearest Neighbour (ANN) search algorithm operates. As in most situations, choosing a vector index involves a tradeoff between accuracy (precision/recall) and speed/throughput.<p>Having scoured the literature through the course of 2023, I find that vector indexing methods can be organized in two levels: a) by their data structures, and b) by their level of compression. These classifications are by no means exhaustive, and many sources disagree on the right way to organize the various indexes, so, this is my best attempt at making sense of it all. Here goes! 😅<h2 id=level-1-data-structures>Level 1: Data structures<a aria-label="Anchor link for: level-1-data-structures" class=zola-anchor href=#level-1-data-structures>#</a></h2><p>It helps to start by organizing indexes based on the data structures that are used to construct them. This is best explored visually.<figure><img alt="Breaking down vector indexes by their underlying data structures" loading=lazy src=vector-db-indexing-types.png><figcaption>Breaking down vector indexes by their underlying data structures</figcaption></figure><h3 id=hash-based-index>Hash-based index<a aria-label="Anchor link for: hash-based-index" class=zola-anchor href=#hash-based-index>#</a></h3><p>Hash-based indexes like LSH (Locally Sensitive Hashing) transform higher dimensional data into lower-dimensional hash codes that aim to keep the original similarity as much as possible. During indexing, the dataset is hashed multiple times to ensure that similar points are more likely to collide (which is the opposite of conventional hashing techniques, where the goal is to minimize collisions). During querying, the query point is also hashed using the same hash functions as used during indexing, and because the similar points are assigned to the same hash bucket, retrieval is very fast. The main advantage of hash-based indexes is that they are very fast while scaling to huge amounts of data, but the downside is that they are not very accurate.<h3 id=tree-based-index>Tree-based index<a aria-label="Anchor link for: tree-based-index" class=zola-anchor href=#tree-based-index>#</a></h3><p>Tree-based index structures allow for rapid searches in high-dimensional spaces, via binary search trees. The tree is constructed in a way that similar data points are more likely to end up in the same subtree, making it much faster to discover approximate nearest neighbours. <strong>Annoy</strong> (Approximate Nearest Neighbours Oh Yeah) was such a method that uses a forest of binary search trees, developed at Spotify. The downside of tree-based indexes is that they perform reasonably well only for low-dimensional data, and are not very accurate for high-dimensional data because they cannot adequately capture the complexity of the data.<h3 id=graph-based-index>Graph-based index<a aria-label="Anchor link for: graph-based-index" class=zola-anchor href=#graph-based-index>#</a></h3><p>Graph-based indexes are based on the idea that data points in vector space form a graph, where the nodes represent the data values, and edges connecting the nodes represent the similarity between the data points. The graph is constructed in a way that similar data points are more likely to be connected by edges, and the ANN search algorithm is designed to traverse the graph in an efficient manner. The main advantage of graph-based indexes, is that they are able to find approximate nearest neighbours in high-dimensional data, while also being memory efficient, increasing performance. HNSW and Vamana, explained below, are examples of graph-based indexes.<p>An extension of graph-based indexes to include concepts from tree-based indexes is NGT<sup class=footnote-reference><a href=#3>3</a></sup> (Neighbourhood Graphs and Trees). Developed by Yahoo! Japan Corporation, it performs two constructions during indexing: one that transforms a dense kNN graph into a bidirectional graph, and another that incrementally constructs a navigable small world (NSW) graph. Where it differs from pure graph-based indexes is in its use of range search via a tree-like structure (Vantage-point, or ‘VP’ trees), a variant of greedy search during graph construction.<p>Because both constructions result in nodes that have a high outward degree, to avoid combinatorial explosion, the seed vertex from which the search originates uses the range search to make the traversal more efficient. This makes NGT a hybrid graph and tree-based index.<h3 id=inverted-file-index>Inverted file index<a aria-label="Anchor link for: inverted-file-index" class=zola-anchor href=#inverted-file-index>#</a></h3><p>Inverted file index (IVF) divides the vector space into a number of tessellated cells, called Voronoi diagrams – these reduce the search space in the same way that clustering does. In order to find the nearest neighbours, the ANN algorithm must simply locate the centroid of the nearest Voronoi cell, and then search only within that cell.<p>The benefit of IVF is that it helps design ANN algorithms that rapidly narrow down on the similarity region of interest, but the disadvantage in its raw form is that the quantization step involved in tessellating the vector space can be slow for very large amounts of data. As a result, IVF is commonly combined with quantization methods like product quantization (PQ) to improve performance, described below.<h2 id=level-2-compression>Level 2: Compression<a aria-label="Anchor link for: level-2-compression" class=zola-anchor href=#level-2-compression>#</a></h2><p>The second level on which indexes can be organized is their compression level: a “flat” or brute force index is one that stores vectors in their unmodified form. When a query vector is received, it is exhaustively compared against each and every vector in the database, as shown in the simplified example below in 3-D space. In essence, using such an index would be like doing a kNN search, where the returned results are exact matches with the $k$ nearest neighbouring vectors. As you can imagine, the time required to return results would increase linearly with the size of the data, making it impractical when applied on a dataset with more than a few hundred thousand vectors.<figure><img alt="Flat index for kNN (exhaustive) search: Image inspired by <a href='https://www.pinecone.io/learn/series/faiss/vector-indexes/'>Pinecone blog</a>" loading=lazy src=vector-db-flat-index.png><figcaption>Flat index for kNN (exhaustive) search: Image inspired by <a href=https://www.pinecone.io/learn/series/faiss/vector-indexes/>Pinecone blog</a></figcaption></figure><p>The solution to improve search efficiency, at the cost of some accuracy in the retrieval, is compression. This process is called <em>quantization</em>, where the underlying vectors in the index are broken into chunks made up of fewer bytes (typically via converting floats to integers) to reduce memory consumption and computational cost during search.<figure><img loading=lazy src=vector-db-indexing-compression.png></figure><h3 id=flat-indexes>Flat indexes<a aria-label="Anchor link for: flat-indexes" class=zola-anchor href=#flat-indexes>#</a></h3><p>When using ANN (non-exhaustive) search, an existing index like IVF or HNSW is termed “flat” when it directly calculates the distance between the query vector and the database vectors in their raw form. To differentiate this from the quantized variants. When used this way, they are called IVF-Flat, HNSW-Flat, and so on.<h3 id=quantized-indexes>Quantized indexes<a aria-label="Anchor link for: quantized-indexes" class=zola-anchor href=#quantized-indexes>#</a></h3><p>A quantized index is one that combines an existing index (IVF, HNSW, Vamana) with compression methods like quantization to reduce the memory footprint and to speed up search. The quantization is typically one of two types<sup class=footnote-reference><a href=#4>4</a></sup>: Scalar Quantization (SQ), or Product Quantization (PQ). SQ converts the floating-point numbers in a vector to integers (which are much smaller in size in bytes) by symmetrically dividing the vector into bins that account for the minimum and maximum value in each dimension.<p>PQ is a more sophisticated method that considers the distribution of values along each vector dimension, performing <em>both</em> compression and data reduction<sup class=footnote-reference><a href=#4>4</a></sup>: The idea behind PQ is to decompose a larger dimensional vector space into a cartesian product of smaller dimensional subspaces by quantizing each subspace into its own clusters – vectors are represented by short codes, such that the distances between them can be efficiently estimated from their codes, termed <em>reproduction values</em>. An asymmetric binning procedure is used (unlike SQ), which increases precision<sup class=footnote-reference><a href=#5>5</a></sup>, as it incorporates the distribution of vectors within each subspace as part of the approximate distance estimation. However, there is a trade-off as it does reduce recall quite significantly<sup class=footnote-reference><a href=#6>6</a></sup>.<hr><h2 id=popular-indexes>Popular indexes<a aria-label="Anchor link for: popular-indexes" class=zola-anchor href=#popular-indexes>#</a></h2><p>Among all the indexing methods listed so far, most purpose-built vector databases implement only a select few of them. This is a very rapidly evolving space 🔥, so a lot of information here may be out of date when you’re reading this. It’s highly recommended you check out the latest documentation of the databases you’re interested in, to see what indexes they support. In this section, I’ll focus on a few popular and upcoming indexes that multiple vendors are focusing on.<h3 id=ivf-pq>IVF-PQ<a aria-label="Anchor link for: ivf-pq" class=zola-anchor href=#ivf-pq>#</a></h3><p>IVF-PQ is a composite index available in databases like Milvus and LanceDB. The IVF part of the index is used to narrow down the search space, and the PQ part is used to speed up the distance calculation between the query vector and the database vectors, and to reduce the memory requirements by quantizing the vectors. The great part about combining the two is that the speed is massively improved due to the PQ component, and IVF component helps improve the recall (that is normally compromised) by the PQ component alone.<p>The PQ component can be broken down as per the diagram below. Each vector representing a data point consists of a fixed number of dimensions $d$ (of the order of hundreds or thousands, depending on the embedding model used upstream). Because storing these many 32 or 64-bit floating point numbers can be quite expensive on a large dataset, product quantization approaches this problem in two stages: the first stage is a coarse quantization stage where the vector is divided into $m$ subvectors, each of dimension $d/m$, and each subvector is assigned a quantized value (termed “reproduction value”) that maps the original vectors to the centroid of the points in that subspace<sup class=footnote-reference><a href=#7>7</a></sup>.<p>The second stage is similar to k-means clustering, where a “codebook” of values is learned by minimizing the distance between the original vector and the quantized vector centroids. By mapping a large, high-dimensional vector into smaller, lower-dimensional subvectors, it is only the codebook of quantized values that is stored, making the memory footprint far smaller.<figure><img alt="Product quantization: Image inspired by <a href='https://www.pinecone.io/learn/series/faiss/product-quantization/'>Pinecone blog</a>" loading=lazy src=vector-db-pq.png><figcaption>Product quantization: Image inspired by <a href=https://www.pinecone.io/learn/series/faiss/product-quantization/>Pinecone blog</a></figcaption></figure><p>IVF is then applied to the PQ vector space – for each point in the space there is a corresponding region, called a <a rel="nofollow noreferrer" href=https://en.wikipedia.org/wiki/Voronoi_diagram>Voronoi cell</a>, consisting of all points in the space closer to the source point (seed) than to any other. These seed points are used to create an inverted index that correlates each centroid with a list of vectors in the space.<figure><img loading=lazy src=vector-db-ivf-cells.png></figure><p>Depending on where the query vector lands, it may be close to the border of multiple Voronoi cells, making it ambiguous which cells to return nearest neighbours from, leading to the <em>edge problem</em>. As a result, IVF-PQ indexes involve setting an additional parameter, <code>n_probes</code>, that tells the search algorithm to expand outward to the number of cells specified by the <code>n_probes</code> parameter.<p>To effectively build an IVF-PQ index, it is necessary to make two choices: the number of subvectors for the PQ step, and the number of partitions for the IVF step. Larger the number of subvectors, the smaller each subspace is, reducing information loss due to compression. However, a larger number of subvectors also results in more I/O and computation within each PQ step, so this number must be minimized to keep computational costs low.<p>Similarly, the number of partitions in the IVF step must be chosen to balance the trade-off between recall and search speed. The limiting case of number of partitions being equal to the number of vectors in the dataset is <em>brute force search</em>, which is the most accurate (recall of 1), but it essentially makes it an IVF-Flat index.<p>Indeed, the LanceDB IVF-PQ index docs<sup class=footnote-reference><a href=#8>8</a></sup> describe exactly these kinds of trade-offs when creating an index, so it’s worth going deeper into these concepts to understand how to apply them in a real world scenario.<h3 id=hnsw>HNSW<a aria-label="Anchor link for: hnsw" class=zola-anchor href=#hnsw>#</a></h3><p>Hierarchical Navigable Small-World (HNSW) graphs is among the most popular algorithms for building vector indexes – as of writing this post, nearly <em>every</em> database vendor out there uses it as the primary option. It’s also among the most intuitive algorithms out there, and it’s highly recommended that you give the <a rel="nofollow noreferrer" href=https://arxiv.org/abs/1603.09320>original paper</a> that introduced it, a read.<p>At a high level, HNSW builds on top of the <a rel="nofollow noreferrer" href=https://en.wikipedia.org/wiki/Small-world_network>small world graph</a> phenomenon, which states that even though most nodes in a graph are not neighbours of each other, the <em>neighbours</em> of any given nodes are likely to be neighbours of each other, regardless of the size of the graph. Essentially, any node in the graph can be reached from every other node by a relatively small number of steps. In the example below, the solid-filled blue and red nodes can be reached from each other in just 3 hops, even though they might be perceived as distant in the graph.<figure><img loading=lazy src=vector-db-small-world-graph.png></figure><p>The vector space in which your data lives can also be thought of as a <em>Navigable</em> Small World (NSW) graph, where the nodes represent the data points, and edges represent similarities, i.e., numbers that describe how close two nodes are to each other in vector space. NSW works by constructing an undirected graph that ensures global connectivity, i.e., any node can be reached in the graph given an arbitrary entry point. Long edges (connecting nodes that are far apart and require many traversals) are formed first, and short edges (which connect nearby nodes) are formed later on. The long edges improve search <em>efficiency</em> and the short edges improve search <em>accuracy</em><sup class=footnote-reference><a href=#3>3</a></sup>. The nearest nodes to a given query vector can be found by traversing this graph.<figure><img loading=lazy src=vector-db-nsw.png></figure><p>One of the problems with an NSW graph is that it is flat – certain nodes can create dense “traffic hubs”, reducing the efficiency of the traversal and causing the search complexity of the method to be poly-logarithmic<sup class=footnote-reference><a href=#9>9</a></sup>. HNSW addresses this issue via a <em>hierarchical</em> graph structure and also fixes the upper bound of each node’s number of neighbours, reducing the search complexity to logarithmic<sup class=footnote-reference><a href=#9>9</a></sup>. The basic idea is to separate nearest neighbours into layers in the graph based on their distance scale. The long edges in the graph are kept in the top layers (which is the sparsest layer), with each layer below containing edges that are shorter-distance than the layers above it. The lowest layer forms the complete graph, and the search is performed from top to bottom. If all layers are “collapsed” into one another, the HNSW graph is essentially an NSW graph.<figure><img loading=lazy src=vector-db-hnsw.png></figure><p>The image above shows how, given an arbitrary entry point at the top layer, it’s possible to rapidly traverse across the graph, dropping one layer at a time, until the nearest neighbour to the query vector is found.<p>The biggest strength of HNSW over IVF is that it is able to find approximate nearest neighbours in complex, high-dimensional vector space with a high degree of recall. In fact, at the time of its release ~2019, it produced state-of-the-art results on benchmark datasets specifically with regard to improving recall while also being fast, explaining its immense popularity. However, it is not as memory efficient, unless it is combined with methods like PQ to compress the vectors at search time. Databases like Qdrant and Weaviate, typically implement composite indexes that involve quantization, like HNSW-PQ<sup class=footnote-reference><a href=#10>10</a></sup> for these reasons, while also offering tuning knobs to adjust the trade-off between recall and query latency.<h3 id=vamana>Vamana<a aria-label="Anchor link for: vamana" class=zola-anchor href=#vamana>#</a></h3><p>Vamana is among the most recently developed graph-based indexing algorithms, first presented at NeurIPS 2019 by <a rel="nofollow noreferrer" href=https://proceedings.neurips.cc/paper_files/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf>Subramanya et al.</a> in collaboration with Microsoft Research India.<p>The standout features of Vamana are:<ul><li>It was designed from the ground up to work both in-memory (which most indexes are designed to do) as well as <em>on-disk</em>, whose implementation as presented by Microsoft, is termed <a rel="nofollow noreferrer" href=https://github.com/microsoft/DiskANN>DiskANN</a>. <ul><li>On-disk indexes seem to be proving a huge implementation challenge for vector database vendors, so this is a key feature of Vamana that differentiates it from other algorithms</ul><li>It allows the indexing of datasets that are too large to fit in memory by constructing smaller indexes for overlapping partitions, that can be easily merged into one single index whose query performance is on par with single indexes constructed for the entire dataset<li>It can also be combined with off-the-shelf vector compression methods like PQ, to build a Vamana-PQ index that powers a DiskANN system – the graph index with the full-precision vectors of the dataset are stored on disk, whereas the compressed vectors are cached in memory, achieving the best of both worlds</ul><p>Vamana iteratively constructs a directed graph, first starting off with a random graph, where each node represents a data point in vector space. At the beginning, the graph is well-connected, meaning nearly all nodes are connected to one another. The graph is then optimized using an objective function that aims to maximize the connectivity between nodes that are closest to one another. This is done by pruning most of the random short-range edges, while also adding certain long-range edges that connect nodes that are quite distant from one another (to speed up traversals in the graph).<figure><img loading=lazy src=vector-db-vamana-build.png></figure><p>During query time, the entry point is chosen to be the global centroid. The search rapidly progresses in the right direction via the long-range edges, which allow the algorithm to jump to the ends of the graph and narrow down on the nearest neighbour of the query vector relatively quickly. In the example below, it takes just three hops to traverse from the entry point, which is the global centroid, to the outer edge of the graph, and then to the nearest neighbour.<figure><img loading=lazy src=vector-db-vamana-query.png></figure><p>As you might have observed already, Vamana offers more of an “inside-out” approach to search, as opposed to the “outside-in” approach of HNSW, where the search starts from a random (potentially far out) node in the top layer and progresses inwards.<p>There are not many databases that currently (as of 2023) implement the Vamana index, presumably due to the technical challenges with on-disk implementations and their implications on latency and search speed. Milvus<sup class=footnote-reference><a href=#11>11</a></sup>, for now, is the only vendor that has a working, on-disk Vamana index, whereas Weaviate<sup class=footnote-reference><a href=#12>12</a></sup> and LanceDB<sup class=footnote-reference><a href=#13>13</a></sup> currently only have experimental implementations. However, this is a rapidly evolving space, so it’s highly recommended that you follow up on the key vector DB vendors to stay up to date on the latest developments!<h2 id=indexes-in-popular-vector-dbs>Indexes in popular vector DBs<a aria-label="Anchor link for: indexes-in-popular-vector-dbs" class=zola-anchor href=#indexes-in-popular-vector-dbs>#</a></h2><p>As shown in <a href=../vector-db-1/>part 1</a> of this series, most databases implement the HNSW index as the default option.<figure><img loading=lazy src=../vector-db-1/vector-db-indexes.png></figure><p>Databases like Milvus, Weaviate, Qdrant and LanceDB offer simple tuning knobs to control the compression/quantization levels in their Product Quantization components. It’s important to understand the fundamentals of how these indexes work, so that you can make the right choice of database and indexing parameters for your use case.<h2 id=conclusions>Conclusions<a aria-label="Anchor link for: conclusions" class=zola-anchor href=#conclusions>#</a></h2><p>The makers of purpose-built vector databases have spent thousands of man hours fine-tuning and optimizing their indexes and storage layers, so if you have large datasets and require &LT100 ms latency on vector search queries, resorting to full-fledged, scalable open-source databases like Weaviate, Qdrant and Milvus seems like a no-brainer, both from a developer’s and a business’s perspective.<ul><li>A <code>Flat</code> index is one that stores vectors in their unmodified form, and is used for exact kNN search. It is the most accurate, but also the slowest.<li><code>IVF-Flat</code> indexes use inverted file indexes to rapidly narrow down on the search space, which are much faster than brute force search, but they sacrifice some accuracy in the form of recall<li><code>IVF-PQ</code> uses IVF in combination with Product Quantization to compress the vectors, reducing the memory footprint and speeding up search, while being better in recall than a pure <code>PQ</code> index<li><code>HNSW</code> is by far the most popular index, and is often combined with Product Quantization, in the form of <code>HNSW-PQ</code>, to improve search speed and memory efficiency compared to <code>IVF-PQ</code><li><code>Vamana</code> is a relatively new index, designed and optimized for on-disk performance – it offers the promise of storing larger-than-memory vector data while performing as well, and as fast, as <code>HNSW</code> <ul><li>However, it’s still early days and not many databases have made the leap towards implementing it due to the challenges of on-disk performance</ul></ul><p>In my view, however, <a rel="nofollow noreferrer" href=https://lancedb.github.io/lancedb/>LanceDB</a> is among the most exciting databases to watch out for in the coming months. This is because it is the <strong>only</strong> vector database in the market where <em>all vector indexes are disk-based</em>. This is because they are innovating on multiple fronts all at once:<ol><li>Building a new, efficient columnar data format, <a rel="nofollow noreferrer" href=https://github.com/lancedb/lance>Lance</a>, that is aimed at becoming a modern successor to parquet, while also being optimized for vector search <ul><li>It’s because of this highly efficient storage layer that LanceDB is able to proceed with such confidence on the disk-based indexing front, unlike other vendors</ul><li>Embedded (serverless), purpose-built architecture built from the ground up<li>Zero-copy data access, which is a huge performance boost for disk-based indexes<li>Automatic versioning of data without needing additional infrastructure<li>Direct integrations with cloud storage providers like AWS S3 and Azure Blob Storage, making it very easy to integrate with existing data pipelines</ol><p>Regardless of which database you choose for your use case, we can all agree that this is a great time to be alive and experimenting with these tools. I know there was a lot of information in this article, but the references below really helped me understand the internals of vector databases and how they are built from the ground up. I hope you found this post interesting, and let’s keep learning! 🚀<p><strong>Other posts in this series</strong><ul><li><a href=../vector-db-1/>Vector databases (Part 1): What makes each one different?</a><li><a href=../vector-db-2/>Vector databases (Part 2): Understanding their internals</a><li><a href=../vector-db-4/>Vector databases (Part 4): Analyzing the trade-offs</a></ul><hr><div class=footnote-definition id=1><sup class=footnote-definition-label>1</sup><p>Not All Vector Databases Are Made Equal, <a rel="nofollow noreferrer" href=https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696>Dmitry Kan on Medium</a></div><div class=footnote-definition id=2><sup class=footnote-definition-label>2</sup><p>Trillion-scale similarity, <a rel="nofollow noreferrer" href=https://milvus.io/blog/Milvus-Was-Built-for-Massive-Scale-Think-Trillion-Vector-Similarity-Search.md>Milvus blog</a></div><div class=footnote-definition id=3><sup class=footnote-definition-label>3</sup><p>A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search, <a rel="nofollow noreferrer" href=https://arxiv.org/abs/2101.12631>arxiv.org</a></div><div class=footnote-definition id=4><sup class=footnote-definition-label>4</sup><p>Choosing the right vector index, <a rel="nofollow noreferrer" href=https://thesequence.substack.com/p/guest-post-choosing-the-right-vector>Frank Liu on Substack</a></div><div class=footnote-definition id=5><sup class=footnote-definition-label>5</sup><p>Product quantization for nearest neighbor search, <a rel="nofollow noreferrer" href=https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf>J’egou, Douze & Schmid</a></div><div class=footnote-definition id=6><sup class=footnote-definition-label>6</sup><p>Scalar Quantization and Product Quantization, <a rel="nofollow noreferrer" href=https://zilliz.com/blog/scalar-quantization-and-product-quantization>Frank Liu on Zilliz blog</a></div><div class=footnote-definition id=7><sup class=footnote-definition-label>7</sup><p>Product quantization: Compressing high-dimensional vectors by 97%, <a rel="nofollow noreferrer" href=https://www.pinecone.io/learn/series/faiss/product-quantization/>Pinecone blog</a></div><div class=footnote-definition id=8><sup class=footnote-definition-label>8</sup><p>ANN indexes, <a rel="nofollow noreferrer" href=https://lancedb.github.io/lancedb/ann_indexes/>LanceDB docs</a></div><div class=footnote-definition id=9><sup class=footnote-definition-label>9</sup><p>Hierarchical Navigable Small-World graphs paper, <a rel="nofollow noreferrer" href=https://arxiv.org/abs/1603.09320>Malkov & Yashunin</a></div><div class=footnote-definition id=10><sup class=footnote-definition-label>10</sup><p>HNSW + PQ, <a rel="nofollow noreferrer" href=https://weaviate.io/blog/ann-algorithms-hnsw-pq>Weaviate blog</a></div><div class=footnote-definition id=11><sup class=footnote-definition-label>11</sup><p>On-disk index, <a rel="nofollow noreferrer" href=https://milvus.io/docs/disk_index.md#On-disk-Index>Milvus docs</a></div><div class=footnote-definition id=12><sup class=footnote-definition-label>12</sup><p>Vamana vs. HNSW, <a rel="nofollow noreferrer" href=https://weaviate.io/blog/ann-algorithms-vamana-vs-hnsw>Weaviate blog</a></div><div class=footnote-definition id=13><sup class=footnote-definition-label>13</sup><p>Types of index, <a rel="nofollow noreferrer" href=https://lancedb.github.io/lancedb/ann_indexes/#types-of-index>LanceDB docs</a></div></article><div class=giscus></div><script async crossorigin data-category=General data-category-id=DIC_kwDOKyWhTs4CbUSt data-emit-metadata=0 data-input-position=bottom data-lang=en data-loading=lazy data-mapping=pathname data-reactions-enabled=1 data-repo=thedataquarry/thedataquarry.github.io data-repo-id=R_kgDOKyWhTg data-strict=0 data-theme=preferred_color_scheme src=https://giscus.app/client.js></script></div><footer><div class=copyright><p>© 2024 Prashanth Rao</div><div class=credits>Powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>