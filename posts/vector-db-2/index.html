<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Vector databases (2): Understanding their internals | The Data Quarry</title><meta content="Vector databases (2): Understanding their internals" property=og:title><meta content="Prashanth Rao" name=author><meta content=en_US property=og:locale><meta content="A primer on embeddings, semantic similarity and search" name=description><meta content="A primer on embeddings, semantic similarity and search" property=og:description><link href=https://thedataquarry.github.io/posts/vector-db-2/ rel=canonical><meta content=https://thedataquarry.github.io/posts/vector-db-2/ property=og:url><meta content="The Data Quarry" property=og:site_name><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=og:image><meta content=article property=og:type><meta content=2023-07-09T00:00:00+00:00 property=article:published_time><meta " content=summary_large_image name=twitter:card><meta content=https://thedataquarry.github.io/img/dataquarry-banner.png property=twitter:image><meta content="Vector databases (2): Understanding their internals" property=twitter:title><meta content=@tech_optimist name=twitter:site><meta content="A primer on embeddings, semantic similarity and search" name=description><title>Vector databases (2): Understanding their internals</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#376dd9;--primary-pale-color:#698bcf1c;--inline-code-color:#444;--text-color:#444;--text-pale-color:#545967;--bg-color:#fff;--highlight-mark-color:#5f75b045;--callout-note-color:#3e70d6;--callout-important-color:#7a46cd;--callout-warning-color:#d3822b;--callout-alert-color:#d43f3f;--callout-question-color:#3089b5;--callout-tip-color:#35a06b;--main-font:"IBMPlexSans",ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:"IBMPlexMono",ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:750px;--main-max-width:750px;--avatar-size:175px;--paragraph-font-size:16px;--paragraph-line-height:1.5em;--aside-font-size:16px;--img-border-radius:4px;--inline-code-border-radius:2px}body.dark{--primary-color:#689afd;--primary-pale-color:#93acdd1c;--inline-code-color:#d2d2d2;--text-color:#ddd;--text-pale-color:#a0a0a0;--bg-color:#202124;--highlight-mark-color:#5f75b045;--callout-note-color:#698bcf;--callout-important-color:#9374c5;--callout-warning-color:#c99054;--callout-alert-color:#d35757;--callout-question-color:#5091b2;--callout-tip-color:#3ea06f}</style><link href=/main.css rel=stylesheet><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI rel=stylesheet><script crossorigin defer integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js></script><script crossorigin defer integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false})})</script><script async data-website-id=01228822-c189-4b8a-93ba-733d045bf346 src=https://analytics.eu.umami.is/script.js></script><body class=post><script>if(localStorage.getItem('theme')=='dark'){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a href=/>The Data Quarry</a><button aria-label="toggle expand" class=separator id=toggler>::</button><span class="wrap left fold">{</span><a href=/posts>blog</a><span class="wrap-separator fold">,</span><a class=fold href=/talks>talks</a><span class="wrap-separator fold">,</span><a class=fold href=/projects>projects</a><span class="wrap right fold">} ;</span></nav><div id=btns><a rel="noreferrer noopener" aria-label=GitHub href=https://github.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=Twitter href=https://twitter.com/tech_optimist target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Twitter</title><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z" fill=currentColor></path></svg> </a><a rel="noreferrer noopener" aria-label=LinkedIn href=https://www.linkedin.com/in/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill=currentColor></path></svg> </a><a aria-label="Buy me a coffee" rel="noreferrer noopener" href=https://www.buymeacoffee.com/prrao87 target=_blank> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><title>Buy Me A Coffee</title><path d="M20.216 6.415l-.132-.666c-.119-.598-.388-1.163-1.001-1.379-.197-.069-.42-.098-.57-.241-.152-.143-.196-.366-.231-.572-.065-.378-.125-.756-.192-1.133-.057-.325-.102-.69-.25-.987-.195-.4-.597-.634-.996-.788a5.723 5.723 0 00-.626-.194c-1-.263-2.05-.36-3.077-.416a25.834 25.834 0 00-3.7.062c-.915.083-1.88.184-2.75.5-.318.116-.646.256-.888.501-.297.302-.393.77-.177 1.146.154.267.415.456.692.58.36.162.737.284 1.123.366 1.075.238 2.189.331 3.287.37 1.218.05 2.437.01 3.65-.118.299-.033.598-.073.896-.119.352-.054.578-.513.474-.834-.124-.383-.457-.531-.834-.473-.466.074-.96.108-1.382.146-1.177.08-2.358.082-3.536.006a22.228 22.228 0 01-1.157-.107c-.086-.01-.18-.025-.258-.036-.243-.036-.484-.08-.724-.13-.111-.027-.111-.185 0-.212h.005c.277-.06.557-.108.838-.147h.002c.131-.009.263-.032.394-.048a25.076 25.076 0 013.426-.12c.674.019 1.347.067 2.017.144l.228.031c.267.04.533.088.798.145.392.085.895.113 1.07.542.055.137.08.288.111.431l.319 1.484a.237.237 0 01-.199.284h-.003c-.037.006-.075.01-.112.015a36.704 36.704 0 01-4.743.295 37.059 37.059 0 01-4.699-.304c-.14-.017-.293-.042-.417-.06-.326-.048-.649-.108-.973-.161-.393-.065-.768-.032-1.123.161-.29.16-.527.404-.675.701-.154.316-.199.66-.267 1-.069.34-.176.707-.135 1.056.087.753.613 1.365 1.37 1.502a39.69 39.69 0 0011.343.376.483.483 0 01.535.53l-.071.697-1.018 9.907c-.041.41-.047.832-.125 1.237-.122.637-.553 1.028-1.182 1.171-.577.131-1.165.2-1.756.205-.656.004-1.31-.025-1.966-.022-.699.004-1.556-.06-2.095-.58-.475-.458-.54-1.174-.605-1.793l-.731-7.013-.322-3.094c-.037-.351-.286-.695-.678-.678-.336.015-.718.3-.678.679l.228 2.185.949 9.112c.147 1.344 1.174 2.068 2.446 2.272.742.12 1.503.144 2.257.156.966.016 1.942.053 2.892-.122 1.408-.258 2.465-1.198 2.616-2.657.34-3.332.683-6.663 1.024-9.995l.215-2.087a.484.484 0 01.39-.426c.402-.078.787-.212 1.074-.518.455-.488.546-1.124.385-1.766zm-1.478.772c-.145.137-.363.201-.578.233-2.416.359-4.866.54-7.308.46-1.748-.06-3.477-.254-5.207-.498-.17-.024-.353-.055-.47-.18-.22-.236-.111-.71-.054-.995.052-.26.152-.609.463-.646.484-.057 1.046.148 1.526.22.577.088 1.156.159 1.737.212 2.48.226 5.002.19 7.472-.14.45-.06.899-.13 1.345-.21.399-.072.84-.206 1.08.206.166.281.188.657.162.974a.544.544 0 01-.169.364zm-6.159 3.9c-.862.37-1.84.788-3.109.788a5.884 5.884 0 01-1.569-.217l.877 9.004c.065.78.717 1.38 1.5 1.38 0 0 1.243.065 1.658.065.447 0 1.786-.065 1.786-.065.783 0 1.434-.6 1.499-1.38l.94-9.95a3.996 3.996 0 00-1.322-.238c-.826 0-1.491.284-2.26.613z" fill=currentColor></path></svg> </a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><div id=wrapper><div id=blank></div><aside class=blur><nav><ul><li><a class=h2 href=#why-is-everybody-talking-about-vector-dbs-these-days>Why is everybody talking about vector DBs these days?</a> <ul><li><a class=h3 href=#the-age-of-large-language-models-llms>The age of Large Language Models (LLMs)</a><li><a class=h3 href=#the-problem-with-relying-on-llms>The problem with relying on LLMs</a></ul><li><a class=h2 href=#what-are-embeddings>What are embeddings?</a><li><a class=h2 href=#how-are-embeddings-generated>How are embeddings generated?</a><li><a class=h2 href=#storing-the-embeddings-in-vector-databases>Storing the embeddings in vector databases</a><li><a class=h2 href=#how-is-similarity-computed>How is similarity computed?</a> <ul><li><a class=h3 href=#an-example-of-measuring-cosine-distance>An example of measuring cosine distance</a></ul><li><a class=h2 href=#scalable-nearest-neighbour-search>Scalable nearest neighbour search</a> <ul><li><a class=h3 href=#approximate-nearest-neighbours-ann>Approximate nearest neighbours (ANN)</a></ul><li><a class=h2 href=#indexing>Indexing</a><li><a class=h2 href=#putting-it-all-together>Putting it all together</a> <ul><li><a class=h3 href=#storage-layer-and-data-ingestion>Storage layer and data ingestion</a><li><a class=h3 href=#application-layer>Application layer</a></ul><li><a class=h2 href=#extending-vector-databases-to-serve-other-functions>Extending vector databases to serve other functions</a> <ul><li><a class=h3 href=#hybrid-search-systems>Hybrid search systems</a></ul><li><a class=h2 href=#generative-qa-chatting-with-your-data>Generative QA: “Chatting with your data”</a><li><a class=h2 href=#conclusions>Conclusions</a></ul></nav><button aria-label="back to top" id=back-to-top><svg viewbox="0 0 24 24" height=24 width=24 xmlns=http://www.w3.org/2000/svg><path d="M11.9997 10.8284L7.04996 15.7782L5.63574 14.364L11.9997 8L18.3637 14.364L16.9495 15.7782L11.9997 10.8284Z" fill=currentColor></path></svg></button></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Vector databases (2): Understanding their internals</h1><div id=post-info><div id=date><span id=publish>2023-07-09</span><span>Updated: <span id=updated>2023-11-28</span></span></div><div id=tags><a href=https://thedataquarry.github.io/tags/vector-db><span>#</span>vector-db</a></div></div><h2 id=why-is-everybody-talking-about-vector-dbs-these-days>Why is everybody talking about vector DBs these days?<a aria-label="Anchor link for: why-is-everybody-talking-about-vector-dbs-these-days" class=zola-anchor href=#why-is-everybody-talking-about-vector-dbs-these-days>#</a></h2><p>This is the second post in a series on vector databases. As mentioned in <a href=../vector-db-1/>part 1</a> of this series, there’s been a lot of marketing (and unfortunately, hype) related to vector databases in the first half of 2023, and if you’re reading this, you’re likely curious what vector databases are actually doing under the hood, and how search functionality is built on top of efficient vector storage.<p>Before going deeper into what vector DBs are, what can explain this frenzy of activity and investment in this space?<h4 id=the-age-of-large-language-models-llms>The age of Large Language Models (LLMs)<a aria-label="Anchor link for: the-age-of-large-language-models-llms" class=zola-anchor href=#the-age-of-large-language-models-llms>#</a></h4><p>In November 2022, an early demo of ChatGPT (which is OpenAI’s interface to GPT 3.5 and above) was released, following which it quickly became the <a rel="nofollow noreferrer" href=https://www.forbes.com/sites/cindygordon/2023/02/02/chatgpt-is-the-fastest-growing-ap-in-the-history-of-web-applications/>fastest growing application in history</a>, gaining a million users in just 5 days 🤯! Indeed, if you look at the ✨ history on GitHub for some of the major open-source vector database repos, it’s clear that there were sharp spikes in the star counts for some of them <em>after</em> the release of ChatGPT in November 2022 and the subsequent release of <a rel="nofollow noreferrer" href=https://openai.com/blog/chatgpt-plugins>ChatGPT plugins</a> in March 2023. These factors, and their associated posts in websites like Hacker News and popular media<sup class=footnote-reference><a href=#1>1</a></sup>, are a large part of why there’s been such a lot of activity in this space.<figure><img alt="Made with ❤️ by <a href='https://star-history.com/#qdrant/qdrant&weaviate/weaviate&milvus-io/milvus&chroma-core/chroma&Date'>star-history.com</a>" loading=lazy src=vector-db-stars.png><figcaption>Made with ❤️ by <a href=https://star-history.com/#qdrant/qdrant&weaviate/weaviate&milvus-io/milvus&chroma-core/chroma&Date>star-history.com</a></figcaption></figure><h4 id=the-problem-with-relying-on-llms>The problem with relying on LLMs<a aria-label="Anchor link for: the-problem-with-relying-on-llms" class=zola-anchor href=#the-problem-with-relying-on-llms>#</a></h4><p>LLMs are <em>generative</em>, meaning that they produce meaningful, coherent text in a sequential manner based on a user prompt. However, when using LLMs to answer a human’s questions, they often produce irrelevant or factually incorrect results.<ul><li>An LLM often <em>hallucinates</em>, i.e., it fabricates information, such as pointing users to URLs or making up numbers that don’t exist<li>LLMs learn/memorize a compressed version of their training data, and although they learn quite well, they don’t do so <em>perfectly</em> – some information is always “lost” in a model’s internal representation of the data<li>An LLM cannot know facts that occurred after its training was completed</ul><p>Vector databases help address these problems, by functioning as the underlying storage layer that can be efficiently queried by an LLM to retrieve facts. Unlike traditional databases, vector DBs specialize in natively representing data as vectors. As a result, we can now build applications with an LLM sitting on top of a vector storage layer that contains recent, up-to-date, factual data (well past the LLM’s training date) and use them to “ground” the model, alleviating the hallucination problem.<p>Although vector databases (e.g., Vespa, Weaviate, Milvus) have existed well before LLMs, since the release of ChatGPT, the open-source community, as well as marketing teams at the vector DB vendors quickly realized their potential in mainstream use cases like search & retrieval in combination with high-quality text generation. This explains the absolute bonanza of VC funding in the world of vector databases!<h2 id=what-are-embeddings>What are embeddings?<a aria-label="Anchor link for: what-are-embeddings" class=zola-anchor href=#what-are-embeddings>#</a></h2><p>A vector database stores not only the original data (which could be images, audio or text), but also its encoded form: <em>embeddings</em>. These embeddings are essentially lists of numbers (i.e., vectors) that store contextual representations of the data. Intuitively, when we refer to an “embedding”, we are talking about a <strong>compressed</strong>, low-dimensional representation of data (images, text, audio) that actually exists in higher dimensions.<p>Within the storage layer, the database stacks $m$ vectors, each representing a data point using $n$ dimensions, for a total size of $m \times n$. The stacks are typically partitioned via sharding for query performance reasons.<figure><img loading=lazy src=vector-embedding.png></figure><h2 id=how-are-embeddings-generated>How are embeddings generated?<a aria-label="Anchor link for: how-are-embeddings-generated" class=zola-anchor href=#how-are-embeddings-generated>#</a></h2><p>The transformer revolution in NLP<sup class=footnote-reference><a href=#2>2</a></sup> has provided engineers with ample means to generate these compressed representations, or embeddings very efficiently, and at scale.<ul><li>A popular method is via the open source library <code>sentence-transformers</code>, available via the <a href="https://huggingface.co/models?library=sentence-transformers" rel="nofollow noreferrer">Hugging Face model hub</a> or directly from <a rel="nofollow noreferrer" href=https://www.sbert.net/>the source repo</a><li>Another (more expensive) method is to use one of many API services: <ul><li><a rel="nofollow noreferrer" href=https://platform.openai.com/docs/guides/embeddings>OpenAI embeddings API</a><li><a rel="nofollow noreferrer" href=https://cohere.com/embed>Cohere embeddings API</a></ul></ul><p>It’s important to keep in mind that the lower the dimensionality of the underlying vectors, the more compact the representation is in embedding space, which can affect downstream task quality. Sentence Transformers (sbert) provides embedding models with a dimension $n$ in the range of 384, 512 and 768, and the models are completely free and open-source. OpenAI and Cohere embeddings, which require a paid API call to generate them, can be considered higher quality due to a dimensionality of a few thousand. One reason it makes sense to use a paid API to generate embeddings is if your data is multilingual (Cohere is known to possess high-quality multilingual embedding models that <a rel="nofollow noreferrer" href=https://docs.cohere.com/docs/multilingual-language-models#model-performance>are known to perform better</a> than open source variants).<blockquote class="callout warning"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M12.865 3.00017L22.3912 19.5002C22.6674 19.9785 22.5035 20.5901 22.0252 20.8662C21.8732 20.954 21.7008 21.0002 21.5252 21.0002H2.47266C1.92037 21.0002 1.47266 20.5525 1.47266 20.0002C1.47266 19.8246 1.51886 19.6522 1.60663 19.5002L11.1329 3.00017C11.4091 2.52187 12.0206 2.358 12.4989 2.63414C12.651 2.72191 12.7772 2.84815 12.865 3.00017ZM4.20471 19.0002H19.7932L11.9989 5.50017L4.20471 19.0002ZM10.9989 16.0002H12.9989V18.0002H10.9989V16.0002ZM10.9989 9.00017H12.9989V14.0002H10.9989V9.00017Z" fill=currentColor></path></svg></div><div class=content><p><strong>Information</strong><p>The choice of embedding model is typically a trade-off between quality and cost. In most cases, for textual data in English, the open-source <code>sentence-transformers</code> model can be utilized as is for text that isn’t too long (~300-400 words for text sequences). It’s possible to deal with text that’s longer than the context length of <code>sentence-transformers</code>, which requires more external tools, but that topic’s for another post!</div></blockquote><h2 id=storing-the-embeddings-in-vector-databases>Storing the embeddings in vector databases<a aria-label="Anchor link for: storing-the-embeddings-in-vector-databases" class=zola-anchor href=#storing-the-embeddings-in-vector-databases>#</a></h2><p>Because of their amenability to operating in embedding space, vector databases are proving to be very useful for <em>semantic or similarity-based search</em> for multiple forms of data (text, image, audio). In semantic search, the input query sent by the user (typically in natural language) is translated into vector form, in the same embedding space as the data itself, so that the top-k results that are most similar to the input query are returned. A visualization of this is shown below.<figure><img loading=lazy src=embedding-pipeline.png></figure><h2 id=how-is-similarity-computed>How is similarity computed?<a aria-label="Anchor link for: how-is-similarity-computed" class=zola-anchor href=#how-is-similarity-computed>#</a></h2><p>The various vector databases offer different metrics to compute similarity, but for text, the following two metrics are most commonly used:<ul><li><strong>Dot product</strong>: This produces a non-normalized value of an arbitrary magnitude<li><strong>Cosine distance</strong>: This produces a normalized value (between -1 and 1)</ul><h3 id=an-example-of-measuring-cosine-distance>An example of measuring cosine distance<a aria-label="Anchor link for: an-example-of-measuring-cosine-distance" class=zola-anchor href=#an-example-of-measuring-cosine-distance>#</a></h3><p>Consider a simplified example where we vectorize the titles of red and white wines in 2-D space, where the horizontal axis represents red wines and the vertical axis represents white wines. In this space, points that are closer together would represent wines that share similar words or concepts, and points that are further apart do not have that much in common. The cosine distance is defined as the cosine of the angle between the lines connecting the position of each wine in the embedding space to the origin.<p>$$cos \theta = \frac{a^T \cdot b}{|a| \cdot{ |b|}}$$<p>A visualization will make this more intuitive.<figure><img loading=lazy src=wines-cosine.png></figure><p>On the left, the two wines (the Reserve White and the Toscana Red) have very little in common, both in terms of vocabulary and concepts, so they have a cosine distance approaching zero (they are orthogonal in the vector space). On the right, the two Zinfandels from Napa Valley have a lot more in common, so they have a much larger cosine similarity closer to 1. The limiting case here would be a cosine distance of 1; a sentence is always perfectly similar to itself.<p>Of course, in a real situation, the actual data exists in higher dimensional vector space (and has many more axes than just the variety of wine) and cannot be visualized on a 2-D plane, but the same principle of cosine similarity applies.<h2 id=scalable-nearest-neighbour-search>Scalable nearest neighbour search<a aria-label="Anchor link for: scalable-nearest-neighbour-search" class=zola-anchor href=#scalable-nearest-neighbour-search>#</a></h2><p>Once the vectors are generated and stored, when a user submits a search query, the goal of similarity search is to provide the top-k most similar vectors to the input query’s own vector. Once again, we can visualize this in a simplified 2-D space.<figure><img loading=lazy src=knn.png></figure><p>The most naive way to do this would be to compare the query vector with each and every vector in the database, with the so-called k-nearest neighbour (kNN) method. However, <strong>this quickly becomes way too expensive</strong> as we scale to millions (or billions) of data points, as the number of comparisons required keeps increasing linearly with the data.<h3 id=approximate-nearest-neighbours-ann>Approximate nearest neighbours (ANN)<a aria-label="Anchor link for: approximate-nearest-neighbours-ann" class=zola-anchor href=#approximate-nearest-neighbours-ann>#</a></h3><p>Every existing vector database focuses on making search highly efficient, regardless of the size of the dataset, via a class of algorithms called <strong>Approximate Nearest Neighbour</strong> (ANN) search. Instead of performing an exhaustive comparison between every vector in the database, an approximate search is performed for nearest neighbours, resulting in some loss of accuracy in the result (the <em>truly</em> nearest neighbour may not always be returned), but a massive performance gain is possible using ANN algorithms.<h2 id=indexing>Indexing<a aria-label="Anchor link for: indexing" class=zola-anchor href=#indexing>#</a></h2><p>Data is stored in a vector database via <em>indexing</em>, which refers to the act of creating data structures called indexes that allow efficient lookup for vectors by rapidly narrowing down on the search space. The embedding models used typically stores vectors with a dimensionality of the order of $10^2$ or $10^3$, and ANN algorithms attempt to capture the actual complexity of the data as efficiently as possible in time and space.<p>There are many indexing algorithms used in the various vector DBs available, and their details are out of the scope of this post (I’ll be studying these in future posts). But for reference, some of them are listed below.<ul><li>Inverted File Index (IVF)<li>Hierarchical Navigable Small World (HNSW) graphs<li>Vamana (utilized in the DiskANN implementation)</ul><p>In a nutshell, the state-of-the-art in indexing is achieved by newer algorithms like HNSW and Vamana, but only a few database vendors offer the DiskANN implementation of Vamana (as of 2023):<ul><li>Milvus<li>Weaviate (WIP)<li>LanceDB (WIP)</ul><h2 id=putting-it-all-together>Putting it all together<a aria-label="Anchor link for: putting-it-all-together" class=zola-anchor href=#putting-it-all-together>#</a></h2><p>We can combine all the above ideas to form a mental picture of what a vector database actually <em>is</em>.<figure><img loading=lazy src=vector-db-components.png></figure><p>The specifics of how each database vendor achieves scalability (via Kubernetes, sharding, streaming and so on) are not important for practitioners – it’s up to each vendor to architect the system considering the trade-offs between latency, cost and scalability.<h3 id=storage-layer-and-data-ingestion>Storage layer and data ingestion<a aria-label="Anchor link for: storage-layer-and-data-ingestion" class=zola-anchor href=#storage-layer-and-data-ingestion>#</a></h3><ul><li>The data that sits somewhere (locally or on the cloud) is passed to an embedding model, converted to vector form and ingested into the storage layer of a vector DB via the API gateway<li>The data is indexed, during which it’s partitioned/sharded for scalability and faster lookups<li>The query engine is tightly integrated with the storage layer, to allow for rapid retrieval of nearest neighbours via the database’s ANN implementation</ul><h3 id=application-layer>Application layer<a aria-label="Anchor link for: application-layer" class=zola-anchor href=#application-layer>#</a></h3><ul><li>A user sends a query via an application’s UI to the embedding model, which converts the input query to a vector that lies in the same embedding space as the data<li>The vectorized query is sent to the query engine via the API gateway <ul><li>Multiple incoming queries are handled asynchronously, and the top-k results are sent back to the user</ul></ul><h2 id=extending-vector-databases-to-serve-other-functions>Extending vector databases to serve other functions<a aria-label="Anchor link for: extending-vector-databases-to-serve-other-functions" class=zola-anchor href=#extending-vector-databases-to-serve-other-functions>#</a></h2><p>The use case above shows how vector DBs enable semantic search at a scale that was not possible several years ago, unless you were a big tech company with massive resources. However, this is just the tip of the iceberg: vector DBs are used to power a host of downstream functions.<h3 id=hybrid-search-systems>Hybrid search systems<a aria-label="Anchor link for: hybrid-search-systems" class=zola-anchor href=#hybrid-search-systems>#</a></h3><p>In his excellent review post<sup class=footnote-reference><a href=#3>3</a></sup>, Colin Harman describes how a lot of companies, due to the plethora of vector DB marketing material out there today, experience “tunnel vision” when it comes to the search & retrieval landscape. As practitioners, we have to remember that vector databases are not the panacea of search – they are very good at <em>semantic</em> search, but in many cases, traditional keyword search can yield more relevant results and increased user satisfaction<sup class=footnote-reference><a href=#4>4</a></sup>. Why is that? It’s largely to do with the fact that ranking based on metrics like cosine similarity causes results that have a higher similarity score to appear above partial matches that may contain specific input keywords, reducing their relevance to the end user.<p>However, pure keyword search also has its own limitations – in case the user enters a term that is semantically similar to the stored data (but is not exact), potentially useful and relevant results are not returned. As a result of this trade-off, real-world use cases for search & retrieval demand a combination of keyword and vector searches, <strong>of which vector databases form a key component</strong> (because they house the embeddings, enabling semantic similarity search and are able to scale to very large datasets).<p>To summarize the points above:<ul><li><strong>Keyword search</strong>: Finds relevant, useful results when the user <em>knows</em> what they’re looking for and expects results that match exact phrases in their search terms. Does <strong>not</strong> require vector databases.<li><strong>Vector search</strong>: Finds relevant results when the user <em>doesn’t</em> know what exactly they’re looking for. Requires a vector database.<li><strong>Hybrid keyword + vector search</strong>: Typically combines candidate results from full-text keyword and vector searches and re-ranks them using cross-encoder models<sup class=footnote-reference><a href=#5>5</a></sup> (see below). Requires both a document database and a vector database.</ul><p>This can be effectively visualized per the diagram below:<figure><img alt="Diagram inspired by <a href='https://qdrant.tech/articles/hybrid-search/'>Qdrant blog post</a>" loading=lazy src=vector-hybrid-search.png><figcaption>Diagram inspired by <a href=https://qdrant.tech/articles/hybrid-search/>Qdrant blog post</a></figcaption></figure><p>BM25<sup class=footnote-reference><a href=#6>6</a></sup> is the most common indexing algorithm used for keyword search in certain databases (e.g., Elasticsearch, Opensearch, MongoDB). It produces a <em>sparse</em> vector, by considering keyword term frequencies in relation to their inverse document frequency (IDF). In contrast, vector databases typically encode and store text in embeddings represented by <em>dense</em> vectors (none of the terms in the vector are zero, unlike BM25), and this is typically done via a bi-encoder model like BERT, that produces a sentence embedding for a pair of documents, that can then be compared to produce a cosine similarity score.<h4 id=understanding-the-difference-between-bi-encoders-and-cross-encoders>Understanding the difference between bi-encoders and cross-encoders<a aria-label="Anchor link for: understanding-the-difference-between-bi-encoders-and-cross-encoders" class=zola-anchor href=#understanding-the-difference-between-bi-encoders-and-cross-encoders>#</a></h4><p>To effectively perform hybrid search, it becomes necessary to combine search result candidates obtained via BM25 (keyword search) and cosine similarity (vector search), which requires a <em>cross-encoder</em>. This is a downstream step, as shown in the image below, that allows two sentences to be simultaneously passed to an encoder model like BERT. Unlike the bi-encoder that’s used to produce sentence embeddings, a cross-encoder doesn’t produce embeddings, but rather, it allows us to classify a pair of sentences by assigning them a score between 0 and 1, via a softmax layer. This is termed <strong><em>re-ranking</em></strong>, and is a very powerful approach to obtaining results that combine the best of both worlds (keyword + vector search).<figure><img alt="Diagram inspired by <a href='https://www.sbert.net/examples/applications/cross-encoder/README.html'>Sentence transformers docs</a>" loading=lazy src=vector-encoders.png><figcaption>Diagram inspired by <a href=https://www.sbert.net/examples/applications/cross-encoder/README.html>Sentence transformers docs</a></figcaption></figure><blockquote class="callout important"><div class=icon><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22ZM12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20ZM11 15H13V17H11V15ZM11 7H13V13H11V7Z" fill=currentColor></path></svg></div><div class=content><p><strong>Important</strong><p>It should be noted that re-ranking via cross-encoders is an expensive step, as it requires the use of a transformer model during query time. This approach is used when the quality of search is critical to a use case, and requires more compute resources (typically, GPUs) and tuning time to ensure that the application is performing as intended.</div></blockquote><h2 id=generative-qa-chatting-with-your-data>Generative QA: “Chatting with your data”<a aria-label="Anchor link for: generative-qa-chatting-with-your-data" class=zola-anchor href=#generative-qa-chatting-with-your-data>#</a></h2><p>With the advent of powerful LLMs like GPT-4, we can effectively integrate the user experience of an application with clean, factual information stored in a vector DB, allowing the user to query their data via natural language. Because question-answering can involve more than just information retrieval (it may require parts of the data to be analyzed, not just queried), including an agent-based framework like LangChain<sup class=footnote-reference><a href=#7>7</a></sup> in between the application UI and the vector DB can be much more powerful than just plugging into the vector DB directly.<figure><img loading=lazy src=vector-db-qa.png></figure><p>Because vector DBs store the data to be queried as embeddings, and the LLM also encodes the knowledge within it as embeddings, they are a natural pairing when it comes to generative QA applications. The vector DB functions as a knowledge base, and the LLM can query a subset of the data directly in the embedding space. This can be done using the following approach:<ol><li>Human asks a question in natural language via the UI<li>The question’s text is passed to an embedding model (bi-encoder), which then returns a sentence embedding vector<li>The question vector is passed to the vector DB, which returns the top-k most similar results, obtained via an ANN search <ul><li>This step is <strong>crucial</strong>, as it massively narrows down the search space for the LLM, used in the next step</ul><li>An LLM prompt is constructed (based on the developer’s predefined template), converted to an embedding, and passed to the LLM <ul><li>A framework like LangChain makes it convenient to perform this step, as the prompt can be dynamically constructed and the LLM’s native embedding module called without the developer having to write a lot of custom code for each workflow</ul><li>The LLM searches the top-k results for the information, and produces an answer to the question<li>The answer is sent back to the human</ol><p>The above workflow can be extended in many ways – for example, if the user’s question involved some arithmetic on numbers obtained from the database (which LLMs are notoriously bad at), a LangChain agent could determine, first, that arithmetic is required. It would then pass the numerical information extracted from the top-k results to a calculator API, perform the calculations, and then send the answer back to the user. With such composable workflows, it’s easy to see how powerful, generative QA chat interfaces are enabled by vector DBs.<h2 id=conclusions>Conclusions<a aria-label="Anchor link for: conclusions" class=zola-anchor href=#conclusions>#</a></h2><p>There are many other useful applications that can be built combining the inherent power of LLMs and vector DBs. However, it’s important to understand some the underlying limitations of vector DBs:<ul><li>They do not necessarily prioritize exact keyword phrase matches for relevance in search applications.<li>The data being stored and queried in vector databases must fit inside the maximum sequence length of the embedding model used (for BERT-like models, this is no longer than a few hundred words). Currently, the best way to do so is to utilize frameworks like LangChain and LlamaIndex<sup class=footnote-reference><a href=#8>8</a></sup> to chunk or squash the data into a fixed-sized vector that fits into the underlying model’s context.</ul><p>Vector databases are incredibly powerful, but the bottom line, per Harman<sup class=footnote-reference><a href=#3>3</a></sup>, applies to all of them:<blockquote><p>If you’re not very knowledgeable about vector databases, and in general, the search and information retrieval space, the following materials <strong>should NOT</strong> be your primary sources of information for any kind of decision-making on choosing your tech stack:<ul><li>Infrastructure stacks from super-smart VCs<li>Tutorials from popular LLM application frameworks</ul></blockquote><p>Just like in any other domain, clearly defining the business case and studying the tools at hand allows you to combine them effectively to solve real-world problems. In that regard, I hope this series on vector DBs was helpful so far!<p><strong>Other posts in this series</strong><ul><li><a href=../vector-db-1/>Vector databases (Part 1): What makes each one different?</a><li><a href=../vector-db-3/>Vector databases (Part 3): Not all indexes are created equal</a><li><a href=../vector-db-4/>Vector databases (Part 4): Analyzing the trade-offs</a></ul><hr><div class=footnote-definition id=1><sup class=footnote-definition-label>1</sup><p>The rise of vector databases, <a href="https://www.forbes.com/sites/adrianbridgwater/2023/05/19/the-rise-of-vector-databases/?sh=7014b16514a6" rel="nofollow noreferrer">Forbes</a></div><div class=footnote-definition id=2><sup class=footnote-definition-label>2</sup><p>Revolution in NLP is changing the way companies understand text, <a rel="nofollow noreferrer" href=https://techcrunch.com/sponsor/nvidia/how-the-revolution-of-natural-language-processing-is-changing-the-way-companies-understand-text/>TechCrunch</a></div><div class=footnote-definition id=3><sup class=footnote-definition-label>3</sup><p>Beware tunnel vision in AI retrieval: Colin Harman on <a rel="nofollow noreferrer" href=https://colinharman.substack.com/p/beware-tunnel-vision-in-ai-retrieval>Substack</a></div><div class=footnote-definition id=4><sup class=footnote-definition-label>4</sup><p>Vector search for clinical decisions, <a rel="nofollow noreferrer" href=https://colinharman.substack.com/i/126262800/haystack-us-erica-lesyshyn-and-max-irwin-vector-search-for-clinical-decisions>Haystack US 2023</a></div><div class=footnote-definition id=5><sup class=footnote-definition-label>5</sup><p>On hybrid search, <a rel="nofollow noreferrer" href=https://qdrant.tech/articles/hybrid-search/>Qdrant blog</a></div><div class=footnote-definition id=6><sup class=footnote-definition-label>6</sup><p>Okapi BM25, <a rel="nofollow noreferrer" href=https://en.wikipedia.org/wiki/Okapi_BM25>Wikipedia</a></div><div class=footnote-definition id=7><sup class=footnote-definition-label>7</sup><p>LangChain <a rel="nofollow noreferrer" href=https://python.langchain.com/docs/get_started/introduction.html>Python docs</a></div><div class=footnote-definition id=8><sup class=footnote-definition-label>8</sup><p>LlamaIndex <a rel="nofollow noreferrer" href=https://gpt-index.readthedocs.io/en/latest/index.html>docs</a></div></article><div class=giscus></div><script async crossorigin data-category=General data-category-id=DIC_kwDOKyWhTs4CbUSt data-emit-metadata=0 data-input-position=bottom data-lang=en data-loading=lazy data-mapping=pathname data-reactions-enabled=1 data-repo=thedataquarry/thedataquarry.github.io data-repo-id=R_kgDOKyWhTg data-strict=0 data-theme=preferred_color_scheme src=https://giscus.app/client.js></script></div><footer><div class=copyright><p>© 2024 Prashanth Rao</div><div class=credits>Powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>